# @verify.py:25-26 这里 with_fallback=False 就会对不齐. 看起来是 B 帧视频下 seek 有问题? 帮我看看如何能改成
_Exported on 2025/12/4 at GMT+8 3:40:29 from Cursor (2.1.46)_

---

**User**

@verify.py:25-26 这里 with_fallback=False 就会对不齐. 看起来是 B 帧视频下 seek 有问题? 帮我看看如何能改成 False 时也对齐

---

**Cursor**

我来分析一下这个问题。首先让我查看相关的代码实现，特别是 `get_batch` 和 seek 相关的逻辑。

```rust
pub fn update_indices(&mut self) {
    self.curr_dec_idx += 1;
    self.curr_frame = self.curr_dec_idx;  // 这里有问题！
    ...
}
```

现在让我看看 `info.rs` 来了解帧时间信息是如何存储的：

现在我理解问题所在了。问题的核心是：

**在含 B 帧的视频中，解码顺序和显示顺序不同。** 当前代码在 `get_batch` 中使用 seek 后，简单地将 `curr_frame` 递增，但这不适用于 B 帧视频，因为解码出的帧的实际显示索引与解码计数器不一致。

让我创建一个修复方案：

首先修改 `info.rs`，添加 PTS 到帧索引的反向映射：

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to frame index (decode order index)
    pts_to_idx: HashMap<i64, usize>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to frame index (decode order index)
    pts_to_idx: HashMap<i64, usize>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build reverse mapping from PTS to frame index
        let pts_to_idx: HashMap<i64, usize> = frame_times
            .iter()
            .map(|(idx, ft)| (ft.pts, *idx))
            .collect();
        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_idx,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get frame index from PTS value
    pub fn get_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_idx.get(&pts).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

现在修改 `reader.rs` 中的 `get_frame` 方法，使用 PTS 来确定帧索引：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let key_pos = self.locate_keyframes(&frame_index);
        debug!("    - Key pos: {}", key_pos);
        let curr_key_pos = self.locate_keyframes(&self.curr_frame);
        debug!("    - Curr key pos: {}", curr_key_pos);
        if (key_pos == curr_key_pos) & (frame_index >= self.curr_frame) {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = self.get_num_skip(&frame_index);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = self.get_num_skip(&frame_index);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}",
            num_skip, self.curr_frame
        );
        // dont retry more than 2x the number of frames we are supposed to skip
        // just to make sure we get out of the loop
        let mut failsafe = (num_skip * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index`
    /// For B-frame videos, we use PTS to identify the actual frame index
    /// instead of relying on decode order counting.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Use PTS to determine the actual frame index for B-frame compatibility
            let pts = decoded.pts().unwrap_or(-1);
            let actual_frame_idx = self
                .stream_info
                .get_idx_from_pts(pts)
                .unwrap_or(self.curr_frame);

            debug!(
                "Decoded frame: pts={}, actual_idx={}, target={}",
                pts, actual_frame_idx, frame_index
            );

            if &actual_frame_idx == frame_index {
                debug!("Found target frame {}", frame_index);
                rgb_frame = self.decoder.process_frame(&decoded);
            }
            // Update curr_frame to the actual decoded frame index for proper tracking
            self.curr_frame = actual_frame_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // Prefer DTS when available for monotonicity; fallback to PTS.
        // If no timestamp is available for this decode index, fall back to frame-number seeking.
        if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
            let dts = *fr_ts.dts();
            let ts = if dts > 0 { dts } else { *fr_ts.pts() };
            match self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD) {
                Ok(()) => Ok(()),
                Err(_) => self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME),
            }
        } else {
            self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME)
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

还需要修改 `skip_frames` 方法来正确处理 B 帧。同时也需要修改 `seek_accurate` 方法中的逻辑：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let key_pos = self.locate_keyframes(&frame_index);
        debug!("    - Key pos: {}", key_pos);
        let curr_key_pos = self.locate_keyframes(&self.curr_frame);
        debug!("    - Curr key pos: {}", curr_key_pos);
        if (key_pos == curr_key_pos) & (frame_index >= self.curr_frame) {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = self.get_num_skip(&frame_index);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = self.get_num_skip(&frame_index);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index`
    /// For B-frame videos, we use PTS to identify the actual frame index
    /// instead of relying on decode order counting.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Use PTS to determine the actual frame index for B-frame compatibility
            let pts = decoded.pts().unwrap_or(-1);
            let actual_frame_idx = self
                .stream_info
                .get_idx_from_pts(pts)
                .unwrap_or(self.curr_frame);

            debug!(
                "Decoded frame: pts={}, actual_idx={}, target={}",
                pts, actual_frame_idx, frame_index
            );

            if &actual_frame_idx == frame_index {
                debug!("Found target frame {}", frame_index);
                rgb_frame = self.decoder.process_frame(&decoded);
            }
            // Update curr_frame to the actual decoded frame index for proper tracking
            self.curr_frame = actual_frame_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // Prefer DTS when available for monotonicity; fallback to PTS.
        // If no timestamp is available for this decode index, fall back to frame-number seeking.
        if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
            let dts = *fr_ts.dts();
            let ts = if dts > 0 { dts } else { *fr_ts.pts() };
            match self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD) {
                Ok(()) => Ok(()),
                Err(_) => self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME),
            }
        } else {
            self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME)
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

还需要更新 `seek_accurate` 方法的逻辑，因为在 B 帧视频中，当前位置判断需要考虑 B 帧的乱序特性：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let key_pos = self.locate_keyframes(&frame_index);
        debug!("    - Key pos: {}", key_pos);
        let curr_key_pos = self.locate_keyframes(&self.curr_frame);
        debug!("    - Curr key pos: {}", curr_key_pos);
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // For B-frame videos, use curr_dec_idx (decode position) instead of curr_frame 
        // to determine if we can skip forward. This is because curr_frame might be 
        // out of order due to B-frame reordering.
        let can_skip_forward =
            (key_pos == curr_key_pos) && (frame_index >= key_pos) && (self.curr_dec_idx <= frame_index);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos || frame_index < self.curr_dec_idx {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = frame_index.saturating_sub(key_pos);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index`
    /// For B-frame videos, we use PTS to identify the actual frame index
    /// instead of relying on decode order counting.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Use PTS to determine the actual frame index for B-frame compatibility
            let pts = decoded.pts().unwrap_or(-1);
            let actual_frame_idx = self
                .stream_info
                .get_idx_from_pts(pts)
                .unwrap_or(self.curr_frame);

            debug!(
                "Decoded frame: pts={}, actual_idx={}, target={}",
                pts, actual_frame_idx, frame_index
            );

            if &actual_frame_idx == frame_index {
                debug!("Found target frame {}", frame_index);
                rgb_frame = self.decoder.process_frame(&decoded);
            }
            // Update curr_frame to the actual decoded frame index for proper tracking
            self.curr_frame = actual_frame_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // Prefer DTS when available for monotonicity; fallback to PTS.
        // If no timestamp is available for this decode index, fall back to frame-number seeking.
        if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
            let dts = *fr_ts.dts();
            let ts = if dts > 0 { dts } else { *fr_ts.pts() };
            match self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD) {
                Ok(()) => Ok(()),
                Err(_) => self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME),
            }
        } else {
            self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME)
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

现在删除不再需要的 `update_indices` 方法的调用，因为我们已经在 `get_frame` 中直接更新了索引：

`update_indices` 不再被使用了，现在检查一下代码是否有 lint 错误：

现在也更新 `get_frame_after_eof` 方法来确保一致性：

代码修改完成了。现在编译并测试：

编译成功！现在运行测试验证：

现在让我看看完整的测试脚本确认是否测试了 `with_fallback=False` 的情况：

看起来脚本现在用的是 `with_fallback=True`。让我修改测试脚本来测试 `with_fallback=False` 的情况：

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

看起来还是有问题。让我仔细分析一下：

问题的根本在于：
1. **ground truth** 是通过迭代器顺序解码得到的，FFmpeg 解码器内部会对 B 帧重排序，输出是按**显示顺序 (PTS 顺序)** 的
2. 但 `frame_times` 是按 **packet/解码顺序 (DTS 顺序)** 建立的索引

所以当用户请求第 73 帧时，期望的是**第 73 个显示帧**，而不是第 73 个 packet。

让我修复这个映射问题：

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build reverse mapping from PTS to frame index
        let pts_to_idx: HashMap<i64, usize> = frame_times
            .iter()
            .map(|(idx, ft)| (ft.pts, *idx))
            .collect();
        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_idx,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get frame index from PTS value
    pub fn get_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_idx.get(&pts).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values",
            frame_count,
            pts_to_presentation_idx.len()
        );

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

现在更新 `reader.rs` 中的 `get_frame` 方法来使用新的显示顺序索引：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let key_pos = self.locate_keyframes(&frame_index);
        debug!("    - Key pos: {}", key_pos);
        let curr_key_pos = self.locate_keyframes(&self.curr_frame);
        debug!("    - Curr key pos: {}", curr_key_pos);
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // For B-frame videos, use curr_dec_idx (decode position) instead of curr_frame 
        // to determine if we can skip forward. This is because curr_frame might be 
        // out of order due to B-frame reordering.
        let can_skip_forward =
            (key_pos == curr_key_pos) && (frame_index >= key_pos) && (self.curr_dec_idx <= frame_index);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos || frame_index < self.curr_dec_idx {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = frame_index.saturating_sub(key_pos);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation/display order index)
    /// For B-frame videos, we use PTS to identify the actual presentation order index
    /// since decode order != display order for B-frames.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Use PTS to determine the presentation order index for B-frame compatibility
            let pts = decoded.pts().unwrap_or(-1);
            let presentation_idx = self
                .stream_info
                .get_presentation_idx_from_pts(pts)
                .unwrap_or(self.curr_frame);

            debug!(
                "Decoded frame: pts={}, presentation_idx={}, target={}",
                pts, presentation_idx, frame_index
            );

            if &presentation_idx == frame_index {
                debug!("Found target frame {}", frame_index);
                rgb_frame = self.decoder.process_frame(&decoded);
            }
            // Update curr_frame to the presentation index for proper tracking
            self.curr_frame = presentation_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // Prefer DTS when available for monotonicity; fallback to PTS.
        // If no timestamp is available for this decode index, fall back to frame-number seeking.
        if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
            let dts = *fr_ts.dts();
            let ts = if dts > 0 { dts } else { *fr_ts.pts() };
            match self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD) {
                Ok(()) => Ok(()),
                Err(_) => self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME),
            }
        } else {
            self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME)
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

还需要更新 `locate_keyframes` 和 `seek_accurate` 来处理显示顺序索引到解码顺序索引的转换：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (display order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // Convert presentation index to decode index for seeking
        // Keyframes are stored by decode index, so we need to find the keyframe
        // before the decode index corresponding to the target presentation index
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        let key_pos = self.locate_keyframes(&target_decode_idx);
        debug!("    - Target presentation idx: {}, decode idx: {}", frame_index, target_decode_idx);
        debug!("    - Key pos (decode order): {}", key_pos);

        let curr_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(self.curr_frame)
            .unwrap_or(self.curr_dec_idx);
        let curr_key_pos = self.locate_keyframes(&curr_decode_idx);
        debug!("    - Curr key pos: {}", curr_key_pos);
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // For B-frame videos, we need to check if we can skip forward
        // We can skip if we're in the same GOP and haven't passed the target
        let can_skip_forward = (key_pos == curr_key_pos)
            && (frame_index >= self.curr_frame)
            && (self.curr_dec_idx <= target_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = target_decode_idx.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos || target_decode_idx < self.curr_dec_idx {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = target_decode_idx.saturating_sub(key_pos);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation/display order index)
    /// For B-frame videos, we use PTS to identify the actual presentation order index
    /// since decode order != display order for B-frames.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Use PTS to determine the presentation order index for B-frame compatibility
            let pts = decoded.pts().unwrap_or(-1);
            let presentation_idx = self
                .stream_info
                .get_presentation_idx_from_pts(pts)
                .unwrap_or(self.curr_frame);

            debug!(
                "Decoded frame: pts={}, presentation_idx={}, target={}",
                pts, presentation_idx, frame_index
            );

            if &presentation_idx == frame_index {
                debug!("Found target frame {}", frame_index);
                rgb_frame = self.decoder.process_frame(&decoded);
            }
            // Update curr_frame to the presentation index for proper tracking
            self.curr_frame = presentation_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // Prefer DTS when available for monotonicity; fallback to PTS.
        // If no timestamp is available for this decode index, fall back to frame-number seeking.
        if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
            let dts = *fr_ts.dts();
            let ts = if dts > 0 { dts } else { *fr_ts.pts() };
            match self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD) {
                Ok(()) => Ok(()),
                Err(_) => self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME),
            }
        } else {
            self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME)
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

还是不行。让我添加一些调试信息来看看具体问题：

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with just first few indices to simplify debugging
batch_indexs = [63, 78]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with just first few indices to simplify debugging
batch_indexs = [63, 78]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches
        for j in range(max(0, idx-10), min(len(ground_truth), idx+10)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}]")
                break

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

```rust
let mut curr_idx: usize = 0;
while self.decoder.video.receive_frame(&mut decoded).is_ok() {
    if needed.contains(&curr_idx) {
        // process frame
    }
    curr_idx += 1;
}
```

```rust
for (stream, packet) in ictx.packets() {
    // ...
    frame_times.insert(didx, FrameTime::new(pts, dur, dts));
    // ...
    didx += 1;
}
```

很好，现在我看到问题了。帧 63 实际上匹配的是 ground_truth[56]（偏移 7 帧）。这说明 PTS 映射有问题。

让我添加更多调试信息来理解映射关系：

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with just first few indices to simplify debugging
batch_indexs = [63, 78]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
# Get PTS info first
pts_list = vr_rs.get_pts()
print(f"Total frames: {len(vr_rs)}")
print(f"First 20 PTS values: {pts_list[:20]}")

# Check PTS order (are they sorted?)
is_sorted = all(pts_list[i] <= pts_list[i+1] for i in range(len(pts_list)-1))
print(f"PTS values are sorted: {is_sorted}")

ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches
        for j in range(max(0, idx-10), min(len(ground_truth), idx+10)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}]")
                break

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

有意思！PTS 值是**未排序**的，而且有负数值。这解释了偏移问题。让我在 Rust 代码中添加更详细的调试输出来验证映射是否正确：

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values",
            frame_count,
            pts_to_presentation_idx.len()
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

现在我理解问题了！关键问题是：

1. 映射建立正确：按 PTS 排序，presentation idx 7 对应 PTS=0（keyframe）
2. 当 seek 到 keyframe（decode idx 6，PTS=0）后，解码器从 PTS=0 开始输出，对应 presentation idx 7
3. 但 presentation idx 0-6 的帧有**负 PTS**，它们在 keyframe 之前！

这意味着在含 B 帧且有负 PTS 的视频中，seek 到第一个 keyframe 后会丢失开头的几帧。

让我改用更直接的方法：直接用**目标帧的 PTS 值**来匹配帧，而不是依赖复杂的索引映射：

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
    /// Mapping from presentation index to PTS (sorted PTS list)
    presentation_idx_to_pts: Vec<i64>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values",
            frame_count,
            pts_to_presentation_idx.len()
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
    /// Mapping from presentation index to PTS (sorted PTS list)
    presentation_idx_to_pts: Vec<i64>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        // Build presentation index to PTS mapping (sorted PTS list)
        let presentation_idx_to_pts: Vec<i64> = pts_sorted
            .iter()
            .map(|(pts, _)| *pts)
            .collect();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values",
            frame_count,
            pts_to_presentation_idx.len()
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
            presentation_idx_to_pts,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
    /// Mapping from presentation index to PTS (sorted PTS list)
    presentation_idx_to_pts: Vec<i64>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        // Build presentation index to PTS mapping (sorted PTS list)
        let presentation_idx_to_pts: Vec<i64> = pts_sorted
            .iter()
            .map(|(pts, _)| *pts)
            .collect();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values",
            frame_count,
            pts_to_presentation_idx.len()
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
            presentation_idx_to_pts,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    /// Get the target PTS for a presentation index
    pub fn get_pts_for_presentation_idx(&self, presentation_idx: usize) -> Option<i64> {
        self.presentation_idx_to_pts.get(presentation_idx).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

现在更新 `reader.rs` 中的 `get_frame` 方法，使用目标 PTS 进行匹配：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (display order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // Convert presentation index to decode index for seeking
        // Keyframes are stored by decode index, so we need to find the keyframe
        // before the decode index corresponding to the target presentation index
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        let key_pos = self.locate_keyframes(&target_decode_idx);
        debug!("    - Target presentation idx: {}, decode idx: {}", frame_index, target_decode_idx);
        debug!("    - Key pos (decode order): {}", key_pos);

        let curr_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(self.curr_frame)
            .unwrap_or(self.curr_dec_idx);
        let curr_key_pos = self.locate_keyframes(&curr_decode_idx);
        debug!("    - Curr key pos: {}", curr_key_pos);
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // For B-frame videos, we need to check if we can skip forward
        // We can skip if we're in the same GOP and haven't passed the target
        let can_skip_forward = (key_pos == curr_key_pos)
            && (frame_index >= self.curr_frame)
            && (self.curr_dec_idx <= target_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = target_decode_idx.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos || target_decode_idx < self.curr_dec_idx {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = target_decode_idx.saturating_sub(key_pos);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation/display order index)
    /// For B-frame videos, we match frames by their PTS value rather than counting,
    /// since decode order != display order for B-frames.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the target PTS for this presentation index
        let target_pts = self
            .stream_info
            .get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);
            let presentation_idx = self
                .stream_info
                .get_presentation_idx_from_pts(pts)
                .unwrap_or(self.curr_frame);

            debug!(
                "Decoded frame: pts={}, presentation_idx={}, target={} (target_pts={:?})",
                pts, presentation_idx, frame_index, target_pts
            );

            // Match by PTS value for accurate B-frame handling
            let is_target = match target_pts {
                Some(target) => pts == target,
                None => &presentation_idx == frame_index,
            };

            if is_target {
                debug!("Found target frame {} with PTS {}", frame_index, pts);
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update curr_frame to the presentation index for proper tracking
            self.curr_frame = presentation_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // Prefer DTS when available for monotonicity; fallback to PTS.
        // If no timestamp is available for this decode index, fall back to frame-number seeking.
        if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
            let dts = *fr_ts.dts();
            let ts = if dts > 0 { dts } else { *fr_ts.pts() };
            match self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD) {
                Ok(()) => Ok(()),
                Err(_) => self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME),
            }
        } else {
            self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME)
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```python
ground_truth = np.array([frame for frame in vr_rs])
```

```rust
pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
    loop {
        match self.ictx.packets().next() {
            Some((stream, packet)) => {
                if stream.index() == self.stream_index {
                    self.decoder.video.send_packet(&packet)?;
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => continue,
                    };
                }
            }
            // ...
        }
    }}
```

```plaintext
    // ...    match index {        None => Ok(vr.stream_info().get_all_pts(time_base)),        // ...
    }
}
```

```rust
pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
    self.frame_times        .values()        .map(|v| v.pts as f64 * time_base)
        .collect::<Vec<_>>()}
```

我明白了！问题在于**解码器可能没有对 B 帧做重排序**，而是按包的顺序（decode order）输出帧。所以 ground_truth 的索引对应的是**包索引**（decode order），而不是**显示顺序**（presentation order）。

让我修改方案：使用 `frame_times[frame_index].pts` 作为目标 PTS，而不是排序后的 PTS：

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
    /// Mapping from presentation index to PTS (sorted PTS list)
    presentation_idx_to_pts: Vec<i64>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        // Build presentation index to PTS mapping (sorted PTS list)
        let presentation_idx_to_pts: Vec<i64> = pts_sorted
            .iter()
            .map(|(pts, _)| *pts)
            .collect();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values",
            frame_count,
            pts_to_presentation_idx.len()
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
            presentation_idx_to_pts,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    /// Get the target PTS for a presentation index (sorted by PTS)
    pub fn get_pts_for_presentation_idx(&self, presentation_idx: usize) -> Option<i64> {
        self.presentation_idx_to_pts.get(presentation_idx).copied()
    }
    /// Get the PTS for a decode order index (packet order)
    /// This is used when the decoder outputs frames in decode order without reordering
    pub fn get_pts_for_decode_idx(&self, decode_idx: usize) -> Option<i64> {
        self.frame_times.get(&decode_idx).map(|ft| ft.pts)
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (display order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // Convert presentation index to decode index for seeking
        // Keyframes are stored by decode index, so we need to find the keyframe
        // before the decode index corresponding to the target presentation index
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        let key_pos = self.locate_keyframes(&target_decode_idx);
        debug!("    - Target presentation idx: {}, decode idx: {}", frame_index, target_decode_idx);
        debug!("    - Key pos (decode order): {}", key_pos);

        let curr_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(self.curr_frame)
            .unwrap_or(self.curr_dec_idx);
        let curr_key_pos = self.locate_keyframes(&curr_decode_idx);
        debug!("    - Curr key pos: {}", curr_key_pos);
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // For B-frame videos, we need to check if we can skip forward
        // We can skip if we're in the same GOP and haven't passed the target
        let can_skip_forward = (key_pos == curr_key_pos)
            && (frame_index >= self.curr_frame)
            && (self.curr_dec_idx <= target_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = target_decode_idx.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos || target_decode_idx < self.curr_dec_idx {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = target_decode_idx.saturating_sub(key_pos);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator output)
    /// For B-frame videos, we match frames by their PTS value since the decoder
    /// may output frames in decode order (same order as packets/iterator).
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the target PTS for this decode order index
        // This matches what the iterator returns (frame_times is indexed by packet/decode order)
        let target_pts = self
            .stream_info
            .get_pts_for_decode_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={} (target_pts={:?})",
                pts, self.curr_dec_idx, frame_index, target_pts
            );

            // Match by PTS value for accurate frame identification
            let is_target = match target_pts {
                Some(target) => pts == target,
                None => self.curr_dec_idx == *frame_index,
            };

            if is_target {
                debug!("Found target frame {} with PTS {}", frame_index, pts);
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update curr_frame based on decode order
            self.curr_frame = self.curr_dec_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // Prefer DTS when available for monotonicity; fallback to PTS.
        // If no timestamp is available for this decode index, fall back to frame-number seeking.
        if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
            let dts = *fr_ts.dts();
            let ts = if dts > 0 { dts } else { *fr_ts.pts() };
            match self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD) {
                Ok(()) => Ok(()),
                Err(_) => self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME),
            }
        } else {
            self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME)
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

现在也需要更新 `seek_accurate` 来使用解码顺序索引：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is already in decode order (same as iterator output)
        // Find the keyframe before this decode index
        let key_pos = self.locate_keyframes(&frame_index);
        debug!("    - Target decode idx: {}", frame_index);
        debug!("    - Key pos (decode order): {}", key_pos);

        let curr_key_pos = self.locate_keyframes(&self.curr_dec_idx);
        debug!("    - Curr key pos: {}", curr_key_pos);
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // We can skip if we're in the same GOP and haven't passed the target
        let can_skip_forward = (key_pos == curr_key_pos) && (self.curr_dec_idx <= frame_index);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos || frame_index < self.curr_dec_idx {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = frame_index.saturating_sub(key_pos);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator output)
    /// For B-frame videos, we match frames by their PTS value since the decoder
    /// may output frames in decode order (same order as packets/iterator).
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the target PTS for this decode order index
        // This matches what the iterator returns (frame_times is indexed by packet/decode order)
        let target_pts = self
            .stream_info
            .get_pts_for_decode_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={} (target_pts={:?})",
                pts, self.curr_dec_idx, frame_index, target_pts
            );

            // Match by PTS value for accurate frame identification
            let is_target = match target_pts {
                Some(target) => pts == target,
                None => self.curr_dec_idx == *frame_index,
            };

            if is_target {
                debug!("Found target frame {} with PTS {}", frame_index, pts);
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update curr_frame based on decode order
            self.curr_frame = self.curr_dec_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // Prefer DTS when available for monotonicity; fallback to PTS.
        // If no timestamp is available for this decode index, fall back to frame-number seeking.
        if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
            let dts = *fr_ts.dts();
            let ts = if dts > 0 { dts } else { *fr_ts.pts() };
            match self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD) {
                Ok(()) => Ok(()),
                Err(_) => self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME),
            }
        } else {
            self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME)
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

偏移从 7 变成了 6，还是不对。让我添加更详细的调试来看 seek 之后发生了什么：

现在我看到问题了！关键问题是：

1. Seek 之后，解码器输出的帧 PTS 是**重置/归零后的相对值**（0, 10, 20...），不是原始文件的 PTS（569）
2. 所以我们无法用原始 PTS 来匹配帧

**解决方案**：seek 到 keyframe K 后，用**位置计数**来匹配，而不是 PTS。`curr_dec_idx` 应该正确追踪位置。

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is already in decode order (same as iterator output)
        // Find the keyframe before this decode index
        let key_pos = self.locate_keyframes(&frame_index);
        debug!("    - Target decode idx: {}", frame_index);
        debug!("    - Key pos (decode order): {}", key_pos);

        let curr_key_pos = self.locate_keyframes(&self.curr_dec_idx);
        debug!("    - Curr key pos: {}", curr_key_pos);
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // We can skip if we're in the same GOP and haven't passed the target
        let can_skip_forward = (key_pos == curr_key_pos) && (self.curr_dec_idx <= frame_index);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos || frame_index < self.curr_dec_idx {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = frame_index.saturating_sub(key_pos);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator output)
    /// After seeking, we match frames by position counter (curr_dec_idx) since
    /// the decoder may reset PTS values after seek.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode position index
            // After seek to keyframe K, curr_dec_idx starts at K and increments for each frame
            if self.curr_dec_idx == *frame_index {
                debug!("Found target frame {} at curr_dec_idx {}", frame_index, self.curr_dec_idx);
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update indices
            self.curr_frame = self.curr_dec_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // Prefer DTS when available for monotonicity; fallback to PTS.
        // If no timestamp is available for this decode index, fall back to frame-number seeking.
        if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
            let dts = *fr_ts.dts();
            let ts = if dts > 0 { dts } else { *fr_ts.pts() };
            match self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD) {
                Ok(()) => Ok(()),
                Err(_) => self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME),
            }
        } else {
            self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME)
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```plaintext
Decoded frame: pts=10, curr_dec_idx=57...
```

看起来我们找到了正确的 `curr_dec_idx`（63 和 78），但帧内容不匹配。这说明 seek 后解码器输出的帧顺序与从头解码时不同。

让我检查 seek 的实际行为：

问题找到了！Seek 之后解码器输出的帧是按**显示顺序（PTS排序）**的，但 ground_truth 是按**包顺序（decode order）**的。

看 PTS 值：seek 后输出 PTS=0, 10, 20...（有序），但原始文件的包顺序 PTS 不是有序的。

让我检查一下 seek 后 PTS=0 的帧对应原始文件的哪个帧：

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with just first few indices to simplify debugging
batch_indexs = [63, 78]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
# Get PTS info first
pts_list = vr_rs.get_pts()
print(f"Total frames: {len(vr_rs)}")
print(f"First 20 PTS values: {pts_list[:20]}")

# Check PTS order (are they sorted?)
is_sorted = all(pts_list[i] <= pts_list[i+1] for i in range(len(pts_list)-1))
print(f"PTS values are sorted: {is_sorted}")

# Check PTS around keyframe 56
print(f"PTS around keyframe 56 (packets 50-65):")
for i in range(50, 66):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f}")

ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches
        for j in range(max(0, idx-10), min(len(ground_truth), idx+10)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}]")
                break

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

```rust
pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
    self.frame_times
        .values()
        .map(|v| v.pts as f64 * time_base)
        .collect::<Vec<_>>()
}
```

问题变得更清楚了。让我添加调试来看 seek 实际使用的是什么时间戳：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is already in decode order (same as iterator output)
        // Find the keyframe before this decode index
        let key_pos = self.locate_keyframes(&frame_index);
        debug!("    - Target decode idx: {}", frame_index);
        debug!("    - Key pos (decode order): {}", key_pos);

        let curr_key_pos = self.locate_keyframes(&self.curr_dec_idx);
        debug!("    - Curr key pos: {}", curr_key_pos);
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // We can skip if we're in the same GOP and haven't passed the target
        let can_skip_forward = (key_pos == curr_key_pos) && (self.curr_dec_idx <= frame_index);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos || frame_index < self.curr_dec_idx {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = frame_index.saturating_sub(key_pos);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator output)
    /// After seeking, we match frames by position counter (curr_dec_idx) since
    /// the decoder may reset PTS values after seek.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode position index
            // After seek to keyframe K, curr_dec_idx starts at K and increments for each frame
            if self.curr_dec_idx == *frame_index {
                debug!("Found target frame {} at curr_dec_idx {}", frame_index, self.curr_dec_idx);
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update indices
            self.curr_frame = self.curr_dec_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // Prefer DTS when available for monotonicity; fallback to PTS.
        // If no timestamp is available for this decode index, fall back to frame-number seeking.
        if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
            let dts = *fr_ts.dts();
            let pts = *fr_ts.pts();
            let ts = if dts > 0 { dts } else { pts };
            debug!(
                "seek_frame: pos={}, pts={}, dts={}, using_ts={}",
                pos, pts, dts, ts
            );
            match self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD) {
                Ok(()) => Ok(()),
                Err(_) => self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME),
            }
        } else {
            debug!("seek_frame: pos={}, no frame_times entry, using frame number", pos);
            self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME)
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

我发现问题了！Seek 使用 DTS=469 去寻找 keyframe 56，但解码后第一帧的 PTS=0。这说明要么 seek 失败了，要么 FFmpeg 在重置 PTS。

视频有负的 `start_time`（第一帧 PTS=-0.25），可能 FFmpeg 在做时间戳归一化。

让我尝试使用帧号 seek 而不是时间戳 seek：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is already in decode order (same as iterator output)
        // Find the keyframe before this decode index
        let key_pos = self.locate_keyframes(&frame_index);
        debug!("    - Target decode idx: {}", frame_index);
        debug!("    - Key pos (decode order): {}", key_pos);

        let curr_key_pos = self.locate_keyframes(&self.curr_dec_idx);
        debug!("    - Curr key pos: {}", curr_key_pos);
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // We can skip if we're in the same GOP and haven't passed the target
        let can_skip_forward = (key_pos == curr_key_pos) && (self.curr_dec_idx <= frame_index);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            if key_pos < curr_key_pos || frame_index < self.curr_dec_idx {
                debug!("Seeking back to start");
                self.seek_to_start()?;
            }
            debug!("Seeking to key_pos: {}", key_pos);
            self.seek_frame(&key_pos)?;
            let num_skip = frame_index.saturating_sub(key_pos);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator output)
    /// After seeking, we match frames by position counter (curr_dec_idx) since
    /// the decoder may reset PTS values after seek.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode position index
            // After seek to keyframe K, curr_dec_idx starts at K and increments for each frame
            if self.curr_dec_idx == *frame_index {
                debug!("Found target frame {} at curr_dec_idx {}", frame_index, self.curr_dec_idx);
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update indices
            self.curr_frame = self.curr_dec_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // First try frame-number based seeking (AVSEEK_FLAG_FRAME) which is more reliable
        // for B-frame videos where timestamps might be normalized or offset
        debug!("seek_frame: pos={}, trying frame-number seek first", pos);
        match self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME | AVSEEK_FLAG_BACKWARD) {
            Ok(()) => {
                debug!("seek_frame: frame-number seek succeeded for pos={}", pos);
                Ok(())
            }
            Err(_) => {
                // Fallback to timestamp-based seeking
                if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
                    let dts = *fr_ts.dts();
                    let pts = *fr_ts.pts();
                    let ts = if dts > 0 { dts } else { pts };
                    debug!(
                        "seek_frame: fallback to timestamp seek pos={}, pts={}, dts={}, using_ts={}",
                        pos, pts, dts, ts
                    );
                    self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD)
                } else {
                    debug!("seek_frame: pos={}, no frame_times entry", pos);
                    Err(ffmpeg::Error::Bug)
                }
            }
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

我明白问题了！`AVSEEK_FLAG_FRAME` 可能是按**输出帧号（presentation order）**来 seek 的，而不是按**包索引（packet order）**。但我们的 keyframe 索引是包索引！

当我们用 `av_seek_frame(56, AVSEEK_FLAG_FRAME)` 时，FFmpeg 可能 seek 到第 56 个输出帧，而不是 packet 56。

让我添加 packet 索引到 presentation 索引的映射，然后用正确的 presentation 索引去 seek：

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
    /// Mapping from presentation index to PTS (sorted PTS list)
    presentation_idx_to_pts: Vec<i64>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        // Build presentation index to PTS mapping (sorted PTS list)
        let presentation_idx_to_pts: Vec<i64> = pts_sorted
            .iter()
            .map(|(pts, _)| *pts)
            .collect();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values",
            frame_count,
            pts_to_presentation_idx.len()
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
            presentation_idx_to_pts,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    /// Get the target PTS for a presentation index (sorted by PTS)
    pub fn get_pts_for_presentation_idx(&self, presentation_idx: usize) -> Option<i64> {
        self.presentation_idx_to_pts.get(presentation_idx).copied()
    }
    /// Get the PTS for a decode order index (packet order)
    /// This is used when the decoder outputs frames in decode order without reordering
    pub fn get_pts_for_decode_idx(&self, decode_idx: usize) -> Option<i64> {
        self.frame_times.get(&decode_idx).map(|ft| ft.pts)
    }
    /// Get presentation index from decode index (packet index)
    /// In B-frame videos, a packet at decode_idx K will produce a frame
    /// at presentation index M (where M is determined by the frame's PTS relative to others)
    pub fn get_presentation_idx_from_decode(&self, decode_idx: usize) -> Option<usize> {
        let pts = self.frame_times.get(&decode_idx)?.pts;
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

现在更新 `seek_accurate` 和 `seek_frame` 来正确处理 presentation order：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (output frame index, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output)
        // Convert to decode order to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        // Convert keyframe's decode index to presentation index for seeking
        let key_presentation_idx = self
            .stream_info
            .get_presentation_idx_from_decode(key_decode_idx)
            .unwrap_or(key_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}",
            frame_index, target_decode_idx
        );
        debug!(
            "    - Keyframe decode idx: {}, presentation idx: {}",
            key_decode_idx, key_presentation_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Compare in presentation order since that's how frames come out
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (frame_index >= key_presentation_idx)
            && (self.curr_frame >= key_presentation_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking back to start and then to key_presentation_idx: {}", key_presentation_idx);
            self.seek_to_start()?;
            self.seek_frame(&key_presentation_idx)?;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator output)
    /// After seeking, we match frames by position counter (curr_dec_idx) since
    /// the decoder may reset PTS values after seek.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode position index
            // After seek to keyframe K, curr_dec_idx starts at K and increments for each frame
            if self.curr_dec_idx == *frame_index {
                debug!("Found target frame {} at curr_dec_idx {}", frame_index, self.curr_dec_idx);
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update indices
            self.curr_frame = self.curr_dec_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        // First try frame-number based seeking (AVSEEK_FLAG_FRAME) which is more reliable
        // for B-frame videos where timestamps might be normalized or offset
        debug!("seek_frame: pos={}, trying frame-number seek first", pos);
        match self.avseekframe(pos, *pos as i64, AVSEEK_FLAG_FRAME | AVSEEK_FLAG_BACKWARD) {
            Ok(()) => {
                debug!("seek_frame: frame-number seek succeeded for pos={}", pos);
                Ok(())
            }
            Err(_) => {
                // Fallback to timestamp-based seeking
                if let Some(fr_ts) = self.stream_info.frame_times().get(pos) {
                    let dts = *fr_ts.dts();
                    let pts = *fr_ts.pts();
                    let ts = if dts > 0 { dts } else { pts };
                    debug!(
                        "seek_frame: fallback to timestamp seek pos={}, pts={}, dts={}, using_ts={}",
                        pos, pts, dts, ts
                    );
                    self.avseekframe(pos, ts, AVSEEK_FLAG_BACKWARD)
                } else {
                    debug!("seek_frame: pos={}, no frame_times entry", pos);
                    Err(ffmpeg::Error::Bug)
                }
            }
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

现在更新 `seek_frame` 和 `get_frame` 来正确使用 presentation order：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (output frame index, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output)
        // Convert to decode order to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        // Convert keyframe's decode index to presentation index for seeking
        let key_presentation_idx = self
            .stream_info
            .get_presentation_idx_from_decode(key_decode_idx)
            .unwrap_or(key_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}",
            frame_index, target_decode_idx
        );
        debug!(
            "    - Keyframe decode idx: {}, presentation idx: {}",
            key_decode_idx, key_presentation_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Compare in presentation order since that's how frames come out
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (frame_index >= key_presentation_idx)
            && (self.curr_frame >= key_presentation_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking back to start and then to key_presentation_idx: {}", key_presentation_idx);
            self.seek_to_start()?;
            self.seek_frame(&key_presentation_idx)?;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator output)
    /// After seeking, we match frames by position counter (curr_dec_idx) since
    /// the decoder may reset PTS values after seek.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode position index
            // After seek to keyframe K, curr_dec_idx starts at K and increments for each frame
            if self.curr_dec_idx == *frame_index {
                debug!("Found target frame {} at curr_dec_idx {}", frame_index, self.curr_dec_idx);
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update indices
            self.curr_frame = self.curr_dec_idx;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a presentation order position (output frame index)
    pub fn seek_frame(&mut self, presentation_pos: &usize) -> Result<(), ffmpeg::Error> {
        // Use frame-number based seeking with presentation order index
        debug!("seek_frame: presentation_pos={}, trying frame-number seek", presentation_pos);
        match self.avseekframe_presentation(presentation_pos, *presentation_pos as i64, AVSEEK_FLAG_FRAME | AVSEEK_FLAG_BACKWARD) {
            Ok(()) => {
                debug!("seek_frame: frame-number seek succeeded for presentation_pos={}", presentation_pos);
                Ok(())
            }
            Err(_) => {
                // Fallback to timestamp-based seeking using the corresponding packet's timestamp
                let decode_idx = self.stream_info
                    .get_decode_idx_from_presentation(*presentation_pos)
                    .unwrap_or(*presentation_pos);
                if let Some(fr_ts) = self.stream_info.frame_times().get(&decode_idx) {
                    let dts = *fr_ts.dts();
                    let pts = *fr_ts.pts();
                    let ts = if dts > 0 { dts } else { pts };
                    debug!(
                        "seek_frame: fallback to timestamp seek presentation_pos={}, decode_idx={}, pts={}, dts={}, using_ts={}",
                        presentation_pos, decode_idx, pts, dts, ts
                    );
                    self.avseekframe_presentation(presentation_pos, ts, AVSEEK_FLAG_BACKWARD)
                } else {
                    debug!("seek_frame: presentation_pos={}, no frame_times entry", presentation_pos);
                    Err(ffmpeg::Error::Bug)
                }
            }
        }
    }
    /// Wrapper for av_seek_frame that updates curr_frame in presentation order
    pub fn avseekframe_presentation(
        &mut self,
        presentation_pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            // Set curr_frame to presentation position (output frame index)
            self.curr_frame = *presentation_pos;
            // Also update curr_dec_idx to corresponding decode position
            self.curr_dec_idx = self.stream_info
                .get_decode_idx_from_presentation(*presentation_pos)
                .unwrap_or(*presentation_pos);
            debug!(
                "avseekframe_presentation: set curr_frame={}, curr_dec_idx={}",
                self.curr_frame, self.curr_dec_idx
            );
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (output frame index, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output)
        // Convert to decode order to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        // Convert keyframe's decode index to presentation index for seeking
        let key_presentation_idx = self
            .stream_info
            .get_presentation_idx_from_decode(key_decode_idx)
            .unwrap_or(key_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}",
            frame_index, target_decode_idx
        );
        debug!(
            "    - Keyframe decode idx: {}, presentation idx: {}",
            key_decode_idx, key_presentation_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Compare in presentation order since that's how frames come out
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (frame_index >= key_presentation_idx)
            && (self.curr_frame >= key_presentation_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking back to start and then to key_presentation_idx: {}", key_presentation_idx);
            self.seek_to_start()?;
            self.seek_frame(&key_presentation_idx)?;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from current frame:{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // Update curr_frame to the found frame index + 1 for next iteration
                            self.curr_frame = *frame_index;
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// After seeking, we match frames by presentation position counter (curr_frame)
    /// since that's how frames are output by the decoder in B-frame videos.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_frame (presentation)={}, target={}",
                pts, self.curr_frame, frame_index
            );

            // Match by presentation order position index
            // After seek to presentation position K, curr_frame starts at K and increments for each output frame
            if self.curr_frame == *frame_index {
                debug!(
                    "Found target frame {} at curr_frame (presentation) {}",
                    frame_index, self.curr_frame
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update presentation index (curr_frame counts output frames in presentation order)
            self.curr_frame += 1;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a presentation order position (output frame index)
    pub fn seek_frame(&mut self, presentation_pos: &usize) -> Result<(), ffmpeg::Error> {
        // Use frame-number based seeking with presentation order index
        debug!("seek_frame: presentation_pos={}, trying frame-number seek", presentation_pos);
        match self.avseekframe_presentation(presentation_pos, *presentation_pos as i64, AVSEEK_FLAG_FRAME | AVSEEK_FLAG_BACKWARD) {
            Ok(()) => {
                debug!("seek_frame: frame-number seek succeeded for presentation_pos={}", presentation_pos);
                Ok(())
            }
            Err(_) => {
                // Fallback to timestamp-based seeking using the corresponding packet's timestamp
                let decode_idx = self.stream_info
                    .get_decode_idx_from_presentation(*presentation_pos)
                    .unwrap_or(*presentation_pos);
                if let Some(fr_ts) = self.stream_info.frame_times().get(&decode_idx) {
                    let dts = *fr_ts.dts();
                    let pts = *fr_ts.pts();
                    let ts = if dts > 0 { dts } else { pts };
                    debug!(
                        "seek_frame: fallback to timestamp seek presentation_pos={}, decode_idx={}, pts={}, dts={}, using_ts={}",
                        presentation_pos, decode_idx, pts, dts, ts
                    );
                    self.avseekframe_presentation(presentation_pos, ts, AVSEEK_FLAG_BACKWARD)
                } else {
                    debug!("seek_frame: presentation_pos={}, no frame_times entry", presentation_pos);
                    Err(ffmpeg::Error::Bug)
                }
            }
        }
    }
    /// Wrapper for av_seek_frame that updates curr_frame in presentation order
    pub fn avseekframe_presentation(
        &mut self,
        presentation_pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            // Set curr_frame to presentation position (output frame index)
            self.curr_frame = *presentation_pos;
            // Also update curr_dec_idx to corresponding decode position
            self.curr_dec_idx = self.stream_info
                .get_decode_idx_from_presentation(*presentation_pos)
                .unwrap_or(*presentation_pos);
            debug!(
                "avseekframe_presentation: set curr_frame={}, curr_dec_idx={}",
                self.curr_frame, self.curr_dec_idx
            );
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (output frame index, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output)
        // Convert to decode order to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        // Convert keyframe's decode index to presentation index for seeking
        let key_presentation_idx = self
            .stream_info
            .get_presentation_idx_from_decode(key_decode_idx)
            .unwrap_or(key_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}",
            frame_index, target_decode_idx
        );
        debug!(
            "    - Keyframe decode idx: {}, presentation idx: {}",
            key_decode_idx, key_presentation_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Compare in presentation order since that's how frames come out
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (frame_index >= key_presentation_idx)
            && (self.curr_frame >= key_presentation_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking back to start and then to key_presentation_idx: {}", key_presentation_idx);
            self.seek_to_start()?;
            self.seek_frame(&key_presentation_idx)?;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // presentation order index
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_frame (presentation):{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // curr_frame has already been incremented past frame_index in get_frame
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// After seeking, we match frames by presentation position counter (curr_frame)
    /// since that's how frames are output by the decoder in B-frame videos.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_frame (presentation)={}, target={}",
                pts, self.curr_frame, frame_index
            );

            // Match by presentation order position index
            // After seek to presentation position K, curr_frame starts at K and increments for each output frame
            if self.curr_frame == *frame_index {
                debug!(
                    "Found target frame {} at curr_frame (presentation) {}",
                    frame_index, self.curr_frame
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update presentation index (curr_frame counts output frames in presentation order)
            self.curr_frame += 1;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a presentation order position (output frame index)
    pub fn seek_frame(&mut self, presentation_pos: &usize) -> Result<(), ffmpeg::Error> {
        // Use frame-number based seeking with presentation order index
        debug!("seek_frame: presentation_pos={}, trying frame-number seek", presentation_pos);
        match self.avseekframe_presentation(presentation_pos, *presentation_pos as i64, AVSEEK_FLAG_FRAME | AVSEEK_FLAG_BACKWARD) {
            Ok(()) => {
                debug!("seek_frame: frame-number seek succeeded for presentation_pos={}", presentation_pos);
                Ok(())
            }
            Err(_) => {
                // Fallback to timestamp-based seeking using the corresponding packet's timestamp
                let decode_idx = self.stream_info
                    .get_decode_idx_from_presentation(*presentation_pos)
                    .unwrap_or(*presentation_pos);
                if let Some(fr_ts) = self.stream_info.frame_times().get(&decode_idx) {
                    let dts = *fr_ts.dts();
                    let pts = *fr_ts.pts();
                    let ts = if dts > 0 { dts } else { pts };
                    debug!(
                        "seek_frame: fallback to timestamp seek presentation_pos={}, decode_idx={}, pts={}, dts={}, using_ts={}",
                        presentation_pos, decode_idx, pts, dts, ts
                    );
                    self.avseekframe_presentation(presentation_pos, ts, AVSEEK_FLAG_BACKWARD)
                } else {
                    debug!("seek_frame: presentation_pos={}, no frame_times entry", presentation_pos);
                    Err(ffmpeg::Error::Bug)
                }
            }
        }
    }
    /// Wrapper for av_seek_frame that updates curr_frame in presentation order
    pub fn avseekframe_presentation(
        &mut self,
        presentation_pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            // Set curr_frame to presentation position (output frame index)
            self.curr_frame = *presentation_pos;
            // Also update curr_dec_idx to corresponding decode position
            self.curr_dec_idx = self.stream_info
                .get_decode_idx_from_presentation(*presentation_pos)
                .unwrap_or(*presentation_pos);
            debug!(
                "avseekframe_presentation: set curr_frame={}, curr_dec_idx={}",
                self.curr_frame, self.curr_dec_idx
            );
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

问题是 seek 实际上可能在跳到 frame 0，而不是 frame 56。让我验证一下，通过检查 seek 后第一个解码帧的 PTS：

问题确认了！Seek 之后第一帧的 PTS=0，说明 FFmpeg 在 seek 后**重置/归一化**了时间戳。视频的 `start_time` 是负数，FFmpeg 可能把 PTS 归一化到从 0 开始。

新方案：seek 后通过**归一化后的 PTS** 来匹配帧。先计算目标帧的预期归一化 PTS：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (output frame index, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output)
        // Convert to decode order to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        // Convert keyframe's decode index to presentation index for seeking
        let key_presentation_idx = self
            .stream_info
            .get_presentation_idx_from_decode(key_decode_idx)
            .unwrap_or(key_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}",
            frame_index, target_decode_idx
        );
        debug!(
            "    - Keyframe decode idx: {}, presentation idx: {}",
            key_decode_idx, key_presentation_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Compare in presentation order since that's how frames come out
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (frame_index >= key_presentation_idx)
            && (self.curr_frame >= key_presentation_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking back to start and then to key_presentation_idx: {}", key_presentation_idx);
            self.seek_to_start()?;
            self.seek_frame(&key_presentation_idx)?;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // presentation order index
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_frame (presentation):{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // curr_frame has already been incremented past frame_index in get_frame
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// After seeking, FFmpeg may normalize PTS values. We match by comparing the decoded
    /// frame's PTS against the expected PTS offset from the seek position.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the original PTS values for the seek base frame and target frame
        let seek_base_pts = self
            .stream_info
            .get_pts_for_presentation_idx(self.curr_frame.saturating_sub(1).max(self.curr_frame))
            .unwrap_or(0);
        let target_original_pts = self
            .stream_info
            .get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;
        let mut first_pts: Option<i64> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            // Track the first frame's PTS to calculate the base offset
            if first_pts.is_none() {
                first_pts = Some(pts);
            }

            debug!(
                "Decoded frame: pts={}, curr_frame (presentation)={}, target={}",
                pts, self.curr_frame, frame_index
            );

            // Match by presentation order position index
            if self.curr_frame == *frame_index {
                debug!(
                    "Found target frame {} at curr_frame (presentation) {}",
                    frame_index, self.curr_frame
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update presentation index
            self.curr_frame += 1;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a presentation order position (output frame index)
    pub fn seek_frame(&mut self, presentation_pos: &usize) -> Result<(), ffmpeg::Error> {
        // Use frame-number based seeking with presentation order index
        debug!("seek_frame: presentation_pos={}, trying frame-number seek", presentation_pos);
        match self.avseekframe_presentation(presentation_pos, *presentation_pos as i64, AVSEEK_FLAG_FRAME | AVSEEK_FLAG_BACKWARD) {
            Ok(()) => {
                debug!("seek_frame: frame-number seek succeeded for presentation_pos={}", presentation_pos);
                Ok(())
            }
            Err(_) => {
                // Fallback to timestamp-based seeking using the corresponding packet's timestamp
                let decode_idx = self.stream_info
                    .get_decode_idx_from_presentation(*presentation_pos)
                    .unwrap_or(*presentation_pos);
                if let Some(fr_ts) = self.stream_info.frame_times().get(&decode_idx) {
                    let dts = *fr_ts.dts();
                    let pts = *fr_ts.pts();
                    let ts = if dts > 0 { dts } else { pts };
                    debug!(
                        "seek_frame: fallback to timestamp seek presentation_pos={}, decode_idx={}, pts={}, dts={}, using_ts={}",
                        presentation_pos, decode_idx, pts, dts, ts
                    );
                    self.avseekframe_presentation(presentation_pos, ts, AVSEEK_FLAG_BACKWARD)
                } else {
                    debug!("seek_frame: presentation_pos={}, no frame_times entry", presentation_pos);
                    Err(ffmpeg::Error::Bug)
                }
            }
        }
    }
    /// Wrapper for av_seek_frame that updates curr_frame in presentation order
    pub fn avseekframe_presentation(
        &mut self,
        presentation_pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            // Set curr_frame to presentation position (output frame index)
            self.curr_frame = *presentation_pos;
            // Also update curr_dec_idx to corresponding decode position
            self.curr_dec_idx = self.stream_info
                .get_decode_idx_from_presentation(*presentation_pos)
                .unwrap_or(*presentation_pos);
            debug!(
                "avseekframe_presentation: set curr_frame={}, curr_dec_idx={}",
                self.curr_frame, self.curr_dec_idx
            );
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```plaintext
seek_frame: pos=56, pts=489, dts=469, using_ts=469
```

让我仔细检查问题。offset 现在是多少：

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with just first few indices to simplify debugging
batch_indexs = [63, 78]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
# Get PTS info first
pts_list = vr_rs.get_pts()
print(f"Total frames: {len(vr_rs)}")
print(f"First 20 PTS values: {pts_list[:20]}")

# Check PTS order (are they sorted?)
is_sorted = all(pts_list[i] <= pts_list[i+1] for i in range(len(pts_list)-1))
print(f"PTS values are sorted: {is_sorted}")

# Check PTS around keyframe 56
print(f"PTS around keyframe 56 (packets 50-65):")
for i in range(50, 66):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f}")

ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches (search wider range)
        for j in range(max(0, idx-60), min(len(ground_truth), idx+60)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}] (offset: {idx - j})")
                break
        else:
            print(f"  -> No match found in range [{max(0,idx-60)}, {min(len(ground_truth), idx+60)})")

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

现在清楚了！偏移正好是 **56**（keyframe 位置）：
- 请求帧 63，得到帧 7（63 - 56 = 7）
- 请求帧 78，得到帧 22（78 - 56 = 22）

这说明 seek **根本没有生效**，解码器还是从头开始输出！让我尝试使用 `ictx.seek()` 方法而不是 `av_seek_frame`：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (output frame index, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output)
        // Convert to decode order to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        // Convert keyframe's decode index to presentation index for seeking
        let key_presentation_idx = self
            .stream_info
            .get_presentation_idx_from_decode(key_decode_idx)
            .unwrap_or(key_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}",
            frame_index, target_decode_idx
        );
        debug!(
            "    - Keyframe decode idx: {}, presentation idx: {}",
            key_decode_idx, key_presentation_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Compare in presentation order since that's how frames come out
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (frame_index >= key_presentation_idx)
            && (self.curr_frame >= key_presentation_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking back to start and then to key_presentation_idx: {}", key_presentation_idx);
            self.seek_to_start()?;
            self.seek_frame(&key_presentation_idx)?;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // presentation order index
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_frame (presentation):{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // curr_frame has already been incremented past frame_index in get_frame
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// After seeking, FFmpeg may normalize PTS values. We match by comparing the decoded
    /// frame's PTS against the expected PTS offset from the seek position.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the original PTS values for the seek base frame and target frame
        let seek_base_pts = self
            .stream_info
            .get_pts_for_presentation_idx(self.curr_frame.saturating_sub(1).max(self.curr_frame))
            .unwrap_or(0);
        let target_original_pts = self
            .stream_info
            .get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;
        let mut first_pts: Option<i64> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            // Track the first frame's PTS to calculate the base offset
            if first_pts.is_none() {
                first_pts = Some(pts);
            }

            debug!(
                "Decoded frame: pts={}, curr_frame (presentation)={}, target={}",
                pts, self.curr_frame, frame_index
            );

            // Match by presentation order position index
            if self.curr_frame == *frame_index {
                debug!(
                    "Found target frame {} at curr_frame (presentation) {}",
                    frame_index, self.curr_frame
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update presentation index
            self.curr_frame += 1;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a presentation order position (output frame index)
    pub fn seek_frame(&mut self, presentation_pos: &usize) -> Result<(), ffmpeg::Error> {
        // Get the corresponding decode index and timestamp
        let decode_idx = self.stream_info
            .get_decode_idx_from_presentation(*presentation_pos)
            .unwrap_or(*presentation_pos);

        if let Some(fr_ts) = self.stream_info.frame_times().get(&decode_idx) {
            let pts = *fr_ts.pts();
            debug!(
                "seek_frame: presentation_pos={}, decode_idx={}, pts={}",
                presentation_pos, decode_idx, pts
            );

            // Use ffmpeg-next's seek API with the PTS timestamp
            // This should be more reliable than raw av_seek_frame
            let time_base = self.ictx.stream(self.stream_index).unwrap().time_base();
            let seek_ts = pts;

            debug!(
                "seek_frame: seeking to pts={} (time_base: {}/{})",
                seek_ts, time_base.numerator(), time_base.denominator()
            );

            // Seek using the ictx API
            match self.ictx.seek(seek_ts, seek_ts..) {
                Ok(()) => {
                    self.avflushbuf()?;
                    self.curr_frame = *presentation_pos;
                    self.curr_dec_idx = decode_idx;
                    debug!(
                        "seek_frame: seek succeeded, set curr_frame={}, curr_dec_idx={}",
                        self.curr_frame, self.curr_dec_idx
                    );
                    Ok(())
                }
                Err(e) => {
                    debug!("seek_frame: ictx.seek failed: {:?}, trying av_seek_frame", e);
                    self.avseekframe_presentation(presentation_pos, pts, AVSEEK_FLAG_BACKWARD)
                }
            }
        } else {
            debug!("seek_frame: presentation_pos={}, no frame_times entry", presentation_pos);
            Err(ffmpeg::Error::Bug)
        }
    }
    /// Wrapper for av_seek_frame that updates curr_frame in presentation order
    pub fn avseekframe_presentation(
        &mut self,
        presentation_pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            // Set curr_frame to presentation position (output frame index)
            self.curr_frame = *presentation_pos;
            // Also update curr_dec_idx to corresponding decode position
            self.curr_dec_idx = self.stream_info
                .get_decode_idx_from_presentation(*presentation_pos)
                .unwrap_or(*presentation_pos);
            debug!(
                "avseekframe_presentation: set curr_frame={}, curr_dec_idx={}",
                self.curr_frame, self.curr_dec_idx
            );
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

啊！问题找到了！看 PTS 值顺序：`[-0.25, -0.183, ..., 0.0, -0.016, ...]`

Index 6 的 PTS=0.0，但 Index 7 的 PTS=-0.016 比它小！这说明迭代器输出的帧顺序是**包顺序（decode order）**，不是**显示顺序（presentation order）**！

所以 ground_truth 的索引是 **decode order**，不是 presentation order。我需要回退到使用 decode order：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // presentation order index
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_frame (presentation):{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // curr_frame has already been incremented past frame_index in get_frame
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// After seeking, FFmpeg may normalize PTS values. We match by comparing the decoded
    /// frame's PTS against the expected PTS offset from the seek position.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the original PTS values for the seek base frame and target frame
        let seek_base_pts = self
            .stream_info
            .get_pts_for_presentation_idx(self.curr_frame.saturating_sub(1).max(self.curr_frame))
            .unwrap_or(0);
        let target_original_pts = self
            .stream_info
            .get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;
        let mut first_pts: Option<i64> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            // Track the first frame's PTS to calculate the base offset
            if first_pts.is_none() {
                first_pts = Some(pts);
            }

            debug!(
                "Decoded frame: pts={}, curr_frame (presentation)={}, target={}",
                pts, self.curr_frame, frame_index
            );

            // Match by presentation order position index
            if self.curr_frame == *frame_index {
                debug!(
                    "Found target frame {} at curr_frame (presentation) {}",
                    frame_index, self.curr_frame
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update presentation index
            self.curr_frame += 1;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a presentation order position (output frame index)
    pub fn seek_frame(&mut self, presentation_pos: &usize) -> Result<(), ffmpeg::Error> {
        // Get the corresponding decode index and timestamp
        let decode_idx = self.stream_info
            .get_decode_idx_from_presentation(*presentation_pos)
            .unwrap_or(*presentation_pos);

        if let Some(fr_ts) = self.stream_info.frame_times().get(&decode_idx) {
            let pts = *fr_ts.pts();
            debug!(
                "seek_frame: presentation_pos={}, decode_idx={}, pts={}",
                presentation_pos, decode_idx, pts
            );

            // Use ffmpeg-next's seek API with the PTS timestamp
            // This should be more reliable than raw av_seek_frame
            let time_base = self.ictx.stream(self.stream_index).unwrap().time_base();
            let seek_ts = pts;

            debug!(
                "seek_frame: seeking to pts={} (time_base: {}/{})",
                seek_ts, time_base.numerator(), time_base.denominator()
            );

            // Seek using the ictx API
            match self.ictx.seek(seek_ts, seek_ts..) {
                Ok(()) => {
                    self.avflushbuf()?;
                    self.curr_frame = *presentation_pos;
                    self.curr_dec_idx = decode_idx;
                    debug!(
                        "seek_frame: seek succeeded, set curr_frame={}, curr_dec_idx={}",
                        self.curr_frame, self.curr_dec_idx
                    );
                    Ok(())
                }
                Err(e) => {
                    debug!("seek_frame: ictx.seek failed: {:?}, trying av_seek_frame", e);
                    self.avseekframe_presentation(presentation_pos, pts, AVSEEK_FLAG_BACKWARD)
                }
            }
        } else {
            debug!("seek_frame: presentation_pos={}, no frame_times entry", presentation_pos);
            Err(ffmpeg::Error::Bug)
        }
    }
    /// Wrapper for av_seek_frame that updates curr_frame in presentation order
    pub fn avseekframe_presentation(
        &mut self,
        presentation_pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            // Set curr_frame to presentation position (output frame index)
            self.curr_frame = *presentation_pos;
            // Also update curr_dec_idx to corresponding decode position
            self.curr_dec_idx = self.stream_info
                .get_decode_idx_from_presentation(*presentation_pos)
                .unwrap_or(*presentation_pos);
            debug!(
                "avseekframe_presentation: set curr_frame={}, curr_dec_idx={}",
                self.curr_frame, self.curr_dec_idx
            );
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // presentation order index
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_frame (presentation):{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // curr_frame has already been incremented past frame_index in get_frame
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// After seeking, FFmpeg may normalize PTS values. We match by comparing the decoded
    /// frame's PTS against the expected PTS offset from the seek position.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the original PTS values for the seek base frame and target frame
        let seek_base_pts = self
            .stream_info
            .get_pts_for_presentation_idx(self.curr_frame.saturating_sub(1).max(self.curr_frame))
            .unwrap_or(0);
        let target_original_pts = self
            .stream_info
            .get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;
        let mut first_pts: Option<i64> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            // Track the first frame's PTS to calculate the base offset
            if first_pts.is_none() {
                first_pts = Some(pts);
            }

            debug!(
                "Decoded frame: pts={}, curr_frame (presentation)={}, target={}",
                pts, self.curr_frame, frame_index
            );

            // Match by presentation order position index
            if self.curr_frame == *frame_index {
                debug!(
                    "Found target frame {} at curr_frame (presentation) {}",
                    frame_index, self.curr_frame
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update presentation index
            self.curr_frame += 1;
            self.curr_dec_idx += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();
            // Use DTS for seeking since that's the decode order timestamp
            let seek_ts = if dts > 0 { dts } else { pts };

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}, seeking to ts={}",
                decode_idx, pts, dts, seek_ts
            );

            // Seek using the ictx API
            match self.ictx.seek(seek_ts, seek_ts..) {
                Ok(()) => {
                    self.avflushbuf()?;
                    self.curr_dec_idx = *decode_idx;
                    self.curr_frame = *decode_idx;
                    debug!(
                        "seek_frame_by_decode_idx: seek succeeded, set curr_dec_idx={}, curr_frame={}",
                        self.curr_dec_idx, self.curr_frame
                    );
                    Ok(())
                }
                Err(e) => {
                    debug!("seek_frame_by_decode_idx: ictx.seek failed: {:?}, trying av_seek_frame", e);
                    self.avseekframe(decode_idx, seek_ts, AVSEEK_FLAG_BACKWARD)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // presentation order index
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_frame (presentation):{}, target: {}",
            num_skip, self.curr_frame, frame_index
        );
        // For B-frame videos, we may need more iterations since frames come out of order
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            // curr_frame has already been incremented past frame_index in get_frame
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current frame is now: {}",
            self.curr_frame
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator/packet order)
    /// After seeking, we count frames in decode order to match the iterator's output.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode order index (packet order, same as iterator)
            if self.curr_dec_idx == *frame_index {
                debug!(
                    "Found target frame {} at curr_dec_idx {}",
                    frame_index, self.curr_dec_idx
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update decode index
            self.curr_dec_idx += 1;
            self.curr_frame = self.curr_dec_idx;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();
            // Use DTS for seeking since that's the decode order timestamp
            let seek_ts = if dts > 0 { dts } else { pts };

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}, seeking to ts={}",
                decode_idx, pts, dts, seek_ts
            );

            // Seek using the ictx API
            match self.ictx.seek(seek_ts, seek_ts..) {
                Ok(()) => {
                    self.avflushbuf()?;
                    self.curr_dec_idx = *decode_idx;
                    self.curr_frame = *decode_idx;
                    debug!(
                        "seek_frame_by_decode_idx: seek succeeded, set curr_dec_idx={}, curr_frame={}",
                        self.curr_dec_idx, self.curr_frame
                    );
                    Ok(())
                }
                Err(e) => {
                    debug!("seek_frame_by_decode_idx: ictx.seek failed: {:?}, trying av_seek_frame", e);
                    self.avseekframe(decode_idx, seek_ts, AVSEEK_FLAG_BACKWARD)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator/packet order)
    /// After seeking, we count frames in decode order to match the iterator's output.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode order index (packet order, same as iterator)
            if self.curr_dec_idx == *frame_index {
                debug!(
                    "Found target frame {} at curr_dec_idx {}",
                    frame_index, self.curr_dec_idx
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update decode index
            self.curr_dec_idx += 1;
            self.curr_frame = self.curr_dec_idx;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();
            // Use DTS for seeking since that's the decode order timestamp
            let seek_ts = if dts > 0 { dts } else { pts };

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}, seeking to ts={}",
                decode_idx, pts, dts, seek_ts
            );

            // Seek using the ictx API
            match self.ictx.seek(seek_ts, seek_ts..) {
                Ok(()) => {
                    self.avflushbuf()?;
                    self.curr_dec_idx = *decode_idx;
                    self.curr_frame = *decode_idx;
                    debug!(
                        "seek_frame_by_decode_idx: seek succeeded, set curr_dec_idx={}, curr_frame={}",
                        self.curr_dec_idx, self.curr_frame
                    );
                    Ok(())
                }
                Err(e) => {
                    debug!("seek_frame_by_decode_idx: ictx.seek failed: {:?}, trying av_seek_frame", e);
                    self.avseekframe(decode_idx, seek_ts, AVSEEK_FLAG_BACKWARD)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

我发现问题了！`ictx.seek()` 期望的是 **AV_TIME_BASE** (微秒)，但我们传的是流的时间戳单位！需要转换：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator/packet order)
    /// After seeking, we count frames in decode order to match the iterator's output.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode order index (packet order, same as iterator)
            if self.curr_dec_idx == *frame_index {
                debug!(
                    "Found target frame {} at curr_dec_idx {}",
                    frame_index, self.curr_dec_idx
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update decode index
            self.curr_dec_idx += 1;
            self.curr_frame = self.curr_dec_idx;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();
            // Use DTS for seeking since that's the decode order timestamp
            let stream_ts = if dts > 0 { dts } else { pts };

            // Get stream time base to convert to AV_TIME_BASE (microseconds)
            // ictx.seek expects timestamp in AV_TIME_BASE units
            let time_base = self.ictx.stream(self.stream_index).unwrap().time_base();
            // Convert: seek_ts_us = stream_ts * (time_base_num / time_base_den) * 1_000_000
            let seek_ts_us = (stream_ts as i64 * time_base.numerator() as i64 * 1_000_000)
                / time_base.denominator() as i64;

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}, stream_ts={}, seek_ts_us={}",
                decode_idx, pts, dts, stream_ts, seek_ts_us
            );

            // Seek using the ictx API with AV_TIME_BASE timestamp
            match self.ictx.seek(seek_ts_us, seek_ts_us..) {
                Ok(()) => {
                    self.avflushbuf()?;
                    self.curr_dec_idx = *decode_idx;
                    self.curr_frame = *decode_idx;
                    debug!(
                        "seek_frame_by_decode_idx: seek succeeded, set curr_dec_idx={}, curr_frame={}",
                        self.curr_dec_idx, self.curr_frame
                    );
                    Ok(())
                }
                Err(e) => {
                    debug!("seek_frame_by_decode_idx: ictx.seek failed: {:?}, trying av_seek_frame", e);
                    // av_seek_frame expects timestamp in stream time base, not AV_TIME_BASE
                    self.avseekframe(decode_idx, stream_ts, AVSEEK_FLAG_BACKWARD)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

seek 看起来成功了但结果还是不对。让我直接用 `av_seek_frame`，它会正确处理时间基准：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator/packet order)
    /// After seeking, we count frames in decode order to match the iterator's output.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode order index (packet order, same as iterator)
            if self.curr_dec_idx == *frame_index {
                debug!(
                    "Found target frame {} at curr_dec_idx {}",
                    frame_index, self.curr_dec_idx
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update decode index
            self.curr_dec_idx += 1;
            self.curr_frame = self.curr_dec_idx;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();
            // Use DTS for seeking since that's the decode order timestamp
            let stream_ts = if dts > 0 { dts } else { pts };

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}, using stream_ts={}",
                decode_idx, pts, dts, stream_ts
            );

            // Use av_seek_frame directly with stream timestamp (in stream time base)
            // AVSEEK_FLAG_BACKWARD seeks to the keyframe at or before the timestamp
            self.avseekframe(decode_idx, stream_ts, AVSEEK_FLAG_BACKWARD)
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with just first few indices to simplify debugging
batch_indexs = [63, 78]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
# Get PTS info first
pts_list = vr_rs.get_pts()
print(f"Total frames: {len(vr_rs)}")
print(f"First 20 PTS values: {pts_list[:20]}")

# Check PTS order (are they sorted?)
is_sorted = all(pts_list[i] <= pts_list[i+1] for i in range(len(pts_list)-1))
print(f"PTS values are sorted: {is_sorted}")

# Check PTS around keyframe 56
print(f"PTS around keyframe 56 (packets 50-65):")
for i in range(50, 66):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f}")

# Check video info
info = vr_rs.get_info()
print(f"\nVideo info:")
print(f"  start_time: {info.get('start_time', 'N/A')}")
print(f"  time_base: {info.get('time_base', 'N/A')}")

ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches (search wider range)
        for j in range(max(0, idx-60), min(len(ground_truth), idx+60)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}] (offset: {idx - j})")
                break
        else:
            print(f"  -> No match found in range [{max(0,idx-60)}, {min(len(ground_truth), idx+60)})")

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

让我检查 seek 之后实际拿到的是哪个 packet。添加更多调试信息：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator/packet order)
    /// After seeking, we count frames in decode order to match the iterator's output.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode order index (packet order, same as iterator)
            if self.curr_dec_idx == *frame_index {
                debug!(
                    "Found target frame {} at curr_dec_idx {}",
                    frame_index, self.curr_dec_idx
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update decode index
            self.curr_dec_idx += 1;
            self.curr_frame = self.curr_dec_idx;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();
            // Use DTS for seeking since that's the decode order timestamp
            let stream_ts = if dts > 0 { dts } else { pts };

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}, using stream_ts={}",
                decode_idx, pts, dts, stream_ts
            );

            // Use av_seek_frame directly with stream timestamp (in stream time base)
            // AVSEEK_FLAG_BACKWARD seeks to the keyframe at or before the timestamp
            self.avseekframe(decode_idx, stream_ts, AVSEEK_FLAG_BACKWARD)
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

问题找到了！Seek 之后第一个 packet 是 PTS=-150（frame 0），不是我们期望的 frame 56！

`av_seek_frame` 去了 keyframe 0 而不是 keyframe 56。这可能是 FFmpeg 的时间戳处理问题。让我尝试不用 `AVSEEK_FLAG_BACKWARD`：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator/packet order)
    /// After seeking, we count frames in decode order to match the iterator's output.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode order index (packet order, same as iterator)
            if self.curr_dec_idx == *frame_index {
                debug!(
                    "Found target frame {} at curr_dec_idx {}",
                    frame_index, self.curr_dec_idx
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update decode index
            self.curr_dec_idx += 1;
            self.curr_frame = self.curr_dec_idx;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

现在 seek 正确到了 keyframe 56（PTS=489）！但解码器输出的第一帧有 PTS=0，这说明解码器在**按 PTS 顺序（显示顺序）**输出帧，而不是包顺序。

问题是 FFmpeg 解码器会对 B 帧重排序。在 seek 后，decoder flush 可能没有完全清空。让我确保 flush 正确执行：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator/packet order)
    /// After seeking, we count frames in decode order to match the iterator's output.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by decode order index (packet order, same as iterator)
            if self.curr_dec_idx == *frame_index {
                debug!(
                    "Found target frame {} at curr_dec_idx {}",
                    frame_index, self.curr_dec_idx
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            // Update decode index
            self.curr_dec_idx += 1;
            self.curr_frame = self.curr_dec_idx;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```plaintext
packet 58: PTS = 0.8317  -> ~499
```

现在解码器的输出是正确的（从 PTS=489 开始），但解码器是按 **PTS 顺序**输出帧的，而不是包顺序！这意味着 output frame 顺序和 packet 顺序不同。

ground_truth 也是按 output 顺序（PTS 顺序）排列的。所以我需要按 PTS 来匹配帧：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (output order index, matching iterator)
    /// The decoder outputs frames in PTS order (presentation order), so we need to
    /// match by PTS to find the correct frame.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the expected PTS for the target frame index
        // frame_times is indexed by packet order, but we need presentation order PTS
        // Use the presentation_idx_to_pts mapping which is sorted by PTS
        let target_pts = self.stream_info.get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, target={}, target_pts={:?}",
                pts, frame_index, target_pts
            );

            // Match by PTS since decoder outputs in PTS order
            if let Some(expected_pts) = target_pts {
                if pts == expected_pts {
                    debug!(
                        "Found target frame {} with PTS {}",
                        frame_index, pts
                    );
                    rgb_frame = self.decoder.process_frame(&decoded);
                }
            }

            self.curr_dec_idx += 1;
            self.curr_frame += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

问题是 `get_pts()` 返回的 PTS 值**未排序**（"PTS values are sorted: False"），说明 ground_truth 的顺序是**包顺序（packet order）**，不是 PTS 排序顺序！

让我验证：迭代器是按包顺序还是 PTS 顺序输出的？

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with just first few indices to simplify debugging
batch_indexs = [63, 78]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
# Get PTS info first
pts_list = vr_rs.get_pts()
print(f"Total frames: {len(vr_rs)}")
print(f"First 20 PTS values: {pts_list[:20]}")

# Check PTS order (are they sorted?)
is_sorted = all(pts_list[i] <= pts_list[i+1] for i in range(len(pts_list)-1))
print(f"PTS values are sorted: {is_sorted}")

# Check what the 63rd frame's PTS actually is in ground_truth order (packet order)
print(f"PTS of ground_truth[63] (packet order): {pts_list[63]:.4f} = {pts_list[63] * 600:.0f} timestamp units")
print(f"PTS of ground_truth[56] (packet order): {pts_list[56]:.4f} = {pts_list[56] * 600:.0f} timestamp units")

# Check PTS around keyframe 56
print(f"PTS around keyframe 56 (packets 50-65):")
for i in range(50, 66):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f}")

# Check video info
info = vr_rs.get_info()
print(f"\nVideo info:")
print(f"  start_time: {info.get('start_time', 'N/A')}")
print(f"  time_base: {info.get('time_base', 'N/A')}")

ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches (search wider range)
        for j in range(max(0, idx-60), min(len(ground_truth), idx+60)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}] (offset: {idx - j})")
                break
        else:
            print(f"  -> No match found in range [{max(0,idx-60)}, {min(len(ground_truth), idx+60)})")

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

明白了！ground_truth[63] 有 PTS=569（包顺序），而我们的 sorted presentation 映射给出了 PTS=559。需要使用**包顺序**的 PTS：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (packet/decode order index, matching iterator)
    /// The decoder outputs frames in PTS order, but the iterator's indices are in packet order.
    /// We match by PTS using the packet order's PTS (from frame_times).
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the expected PTS for the target frame index in PACKET order
        // frame_times is indexed by packet order, which matches the iterator's output order
        let target_pts = self.stream_info.get_pts_for_decode_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, target={}, target_pts={:?}",
                pts, frame_index, target_pts
            );

            // Match by PTS from packet order
            if let Some(expected_pts) = target_pts {
                if pts == expected_pts {
                    debug!(
                        "Found target frame {} with PTS {}",
                        frame_index, pts
                    );
                    rgb_frame = self.decoder.process_frame(&decoded);
                }
            }

            self.curr_dec_idx += 1;
            self.curr_frame += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```rust
if pts == expected_pts {
    rgb_frame = self.decoder.process_frame(&decoded);
}
```

有问题。让我检查迭代器输出的 PTS 顺序是否与 seek 后的一致：

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with just first few indices to simplify debugging
batch_indexs = [63, 78]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
# Get PTS info first
pts_list = vr_rs.get_pts()
print(f"Total frames: {len(vr_rs)}")
print(f"First 20 PTS values: {pts_list[:20]}")

# Check PTS order (are they sorted?)
is_sorted = all(pts_list[i] <= pts_list[i+1] for i in range(len(pts_list)-1))
print(f"PTS values are sorted: {is_sorted}")

# Check what the 63rd frame's PTS actually is in ground_truth order (packet order)
print(f"PTS of ground_truth[63] (packet order): {pts_list[63]:.4f} = {pts_list[63] * 600:.0f} timestamp units")
print(f"PTS of ground_truth[56] (packet order): {pts_list[56]:.4f} = {pts_list[56] * 600:.0f} timestamp units")

# Print PTS sequence for frames 55-70
print("\nPTS sequence for frames 55-70 (packet order):")
for i in range(55, 71):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f} = {pts_list[i] * 600:.0f} ts")

# Check PTS around keyframe 56
print(f"PTS around keyframe 56 (packets 50-65):")
for i in range(50, 66):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f}")

# Check video info
info = vr_rs.get_info()
print(f"\nVideo info:")
print(f"  start_time: {info.get('start_time', 'N/A')}")
print(f"  time_base: {info.get('time_base', 'N/A')}")

ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches (search wider range)
        for j in range(max(0, idx-60), min(len(ground_truth), idx+60)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}] (offset: {idx - j})")
                break
        else:
            print(f"  -> No match found in range [{max(0,idx-60)}, {min(len(ground_truth), idx+60)})")

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

问题是：ground_truth 是按**解码器输出顺序**（PTS 排序）排列的，不是包顺序！所以我需要用 sorted PTS：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a decode order index (packet order, matching iterator)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in decode order (same as iterator/packet order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, Keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // Use decode order indices
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// The decoder outputs frames in PTS order (presentation order), and the iterator's
    /// indices are also in presentation order. We match by sorted PTS.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the expected PTS for the target frame index in PRESENTATION order
        // The iterator outputs frames in PTS sorted order, so ground_truth[N] has
        // the (N+1)th smallest PTS
        let target_pts = self.stream_info.get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, target={}, target_pts={:?}",
                pts, frame_index, target_pts
            );

            // Match by PTS in presentation order (sorted PTS)
            if let Some(expected_pts) = target_pts {
                if pts == expected_pts {
                    debug!(
                        "Found target frame {} with PTS {}",
                        frame_index, pts
                    );
                    rgb_frame = self.decoder.process_frame(&decoded);
                }
            }

            self.curr_dec_idx += 1;
            self.curr_frame += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

也需要更新 `seek_accurate` 来使用 presentation order 找 keyframe：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (iterator output order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output order)
        // Convert to decode index to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}, keyframe decode idx: {}",
            frame_index, target_decode_idx, key_decode_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (target_decode_idx >= self.curr_dec_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            self.seek_to_start()?;
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            // After seeking, we need to decode until we find the frame with the target PTS
            // The keyframe's presentation index might be different from its decode index
            let key_presentation_idx = self
                .stream_info
                .get_presentation_idx_from_decode(key_decode_idx)
                .unwrap_or(key_decode_idx);
            self.curr_frame = key_presentation_idx;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// The decoder outputs frames in PTS order (presentation order), and the iterator's
    /// indices are also in presentation order. We match by sorted PTS.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the expected PTS for the target frame index in PRESENTATION order
        // The iterator outputs frames in PTS sorted order, so ground_truth[N] has
        // the (N+1)th smallest PTS
        let target_pts = self.stream_info.get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, target={}, target_pts={:?}",
                pts, frame_index, target_pts
            );

            // Match by PTS in presentation order (sorted PTS)
            if let Some(expected_pts) = target_pts {
                if pts == expected_pts {
                    debug!(
                        "Found target frame {} with PTS {}",
                        frame_index, pts
                    );
                    rgb_frame = self.decoder.process_frame(&decoded);
                }
            }

            self.curr_dec_idx += 1;
            self.curr_frame += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```rust
debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
self.seek_to_start()?;  // <-- This might be flushing our seek target
self.seek_frame_by_decode_idx(&key_decode_idx)?;
```

问题是 `seek_to_start()` 干扰了后续的 seek。让我移除它：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (iterator output order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output order)
        // Convert to decode index to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}, keyframe decode idx: {}",
            frame_index, target_decode_idx, key_decode_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (target_decode_idx >= self.curr_dec_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            // Directly seek to the keyframe without going to start first
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            // After seeking, we need to decode until we find the frame with the target PTS
            // The keyframe's presentation index might be different from its decode index
            let key_presentation_idx = self
                .stream_info
                .get_presentation_idx_from_decode(key_decode_idx)
                .unwrap_or(key_decode_idx);
            self.curr_frame = key_presentation_idx;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// The decoder outputs frames in PTS order (presentation order), and the iterator's
    /// indices are also in presentation order. We match by sorted PTS.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the expected PTS for the target frame index in PRESENTATION order
        // The iterator outputs frames in PTS sorted order, so ground_truth[N] has
        // the (N+1)th smallest PTS
        let target_pts = self.stream_info.get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, target={}, target_pts={:?}",
                pts, frame_index, target_pts
            );

            // Match by PTS in presentation order (sorted PTS)
            if let Some(expected_pts) = target_pts {
                if pts == expected_pts {
                    debug!(
                        "Found target frame {} with PTS {}",
                        frame_index, pts
                    );
                    rgb_frame = self.decoder.process_frame(&decoded);
                }
            }

            self.curr_dec_idx += 1;
            self.curr_frame += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

seek 还是出问题 - 跳到了 keyframe 0 而不是 56。问题是可能需要先 seek 到开始再往前 seek。让我在需要时才 seek to start：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (iterator output order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output order)
        // Convert to decode index to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}, keyframe decode idx: {}",
            frame_index, target_decode_idx, key_decode_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (target_decode_idx >= self.curr_dec_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            // For backwards seeking, first go to start then seek forward
            // This is more reliable than direct backward seeking
            if key_decode_idx < self.curr_dec_idx {
                debug!("Backward seek detected, going to start first");
                self.seek_to_start()?;
            }
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            // After seeking, we need to decode until we find the frame with the target PTS
            // The keyframe's presentation index might be different from its decode index
            let key_presentation_idx = self
                .stream_info
                .get_presentation_idx_from_decode(key_decode_idx)
                .unwrap_or(key_decode_idx);
            self.curr_frame = key_presentation_idx;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// The decoder outputs frames in PTS order (presentation order), and the iterator's
    /// indices are also in presentation order. We match by sorted PTS.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the expected PTS for the target frame index in PRESENTATION order
        // The iterator outputs frames in PTS sorted order, so ground_truth[N] has
        // the (N+1)th smallest PTS
        let target_pts = self.stream_info.get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, target={}, target_pts={:?}",
                pts, frame_index, target_pts
            );

            // Match by PTS in presentation order (sorted PTS)
            if let Some(expected_pts) = target_pts {
                if pts == expected_pts {
                    debug!(
                        "Found target frame {} with PTS {}",
                        frame_index, pts
                    );
                    rgb_frame = self.decoder.process_frame(&decoded);
                }
            }

            self.curr_dec_idx += 1;
            self.curr_frame += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

问题在于 `can_skip_forward` 条件！当 `curr_dec_idx=0` 时，我们不应该尝试跳过，应该先 seek。让我修复条件：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (iterator output order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output order)
        // Convert to decode index to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}, keyframe decode idx: {}",
            frame_index, target_decode_idx, key_decode_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // We can only skip if we're already at or past the keyframe for the target
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (target_decode_idx >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            // For backwards seeking, first go to start then seek forward
            // This is more reliable than direct backward seeking
            if key_decode_idx < self.curr_dec_idx {
                debug!("Backward seek detected, going to start first");
                self.seek_to_start()?;
            }
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            // After seeking, we need to decode until we find the frame with the target PTS
            // The keyframe's presentation index might be different from its decode index
            let key_presentation_idx = self
                .stream_info
                .get_presentation_idx_from_decode(key_decode_idx)
                .unwrap_or(key_decode_idx);
            self.curr_frame = key_presentation_idx;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (presentation order index, matching iterator output)
    /// The decoder outputs frames in PTS order (presentation order), and the iterator's
    /// indices are also in presentation order. We match by sorted PTS.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the expected PTS for the target frame index in PRESENTATION order
        // The iterator outputs frames in PTS sorted order, so ground_truth[N] has
        // the (N+1)th smallest PTS
        let target_pts = self.stream_info.get_pts_for_presentation_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, target={}, target_pts={:?}",
                pts, frame_index, target_pts
            );

            // Match by PTS in presentation order (sorted PTS)
            if let Some(expected_pts) = target_pts {
                if pts == expected_pts {
                    debug!(
                        "Found target frame {} with PTS {}",
                        frame_index, pts
                    );
                    rgb_frame = self.decoder.process_frame(&decoded);
                }
            }

            self.curr_dec_idx += 1;
            self.curr_frame += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

让我验证迭代器的输出顺序是否真的是 PTS 排序的。添加一个测试：

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with just first few indices to simplify debugging
batch_indexs = [63, 78]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
# Get PTS info first
pts_list = vr_rs.get_pts()
print(f"Total frames: {len(vr_rs)}")
print(f"First 20 PTS values: {pts_list[:20]}")

# Check PTS order (are they sorted?)
is_sorted = all(pts_list[i] <= pts_list[i+1] for i in range(len(pts_list)-1))
print(f"PTS values are sorted: {is_sorted}")

# Check what the 63rd frame's PTS actually is in ground_truth order (packet order)
print(f"PTS of ground_truth[63] (packet order): {pts_list[63]:.4f} = {pts_list[63] * 600:.0f} timestamp units")
print(f"PTS of ground_truth[56] (packet order): {pts_list[56]:.4f} = {pts_list[56] * 600:.0f} timestamp units")

# Print PTS sequence for frames 55-70
print("\nPTS sequence for frames 55-70 (packet order):")
for i in range(55, 71):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f} = {pts_list[i] * 600:.0f} ts")

# Check if iterator output is in PTS sorted order by examining the actual frames
# This is the key test: is ground_truth[N] the frame with (N+1)th smallest PTS?
sorted_pts = sorted(enumerate(pts_list), key=lambda x: x[1])
print("\nFirst 10 frames in sorted PTS order:")
for rank, (packet_idx, pts_val) in enumerate(sorted_pts[:10]):
    print(f"  rank {rank}: packet {packet_idx}, PTS = {pts_val:.4f}")

# Check which packet corresponds to ground_truth[63]
# If iterator outputs in PTS order, ground_truth[63] should match packet at sorted_pts[63]
print(f"\nSorted PTS position 56: packet {sorted_pts[56][0]}, PTS = {sorted_pts[56][1]:.4f}")
print(f"Sorted PTS position 63: packet {sorted_pts[63][0]}, PTS = {sorted_pts[63][1]:.4f}")

# Check PTS around keyframe 56
print(f"PTS around keyframe 56 (packets 50-65):")
for i in range(50, 66):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f}")

# Check video info
info = vr_rs.get_info()
print(f"\nVideo info:")
print(f"  start_time: {info.get('start_time', 'N/A')}")
print(f"  time_base: {info.get('time_base', 'N/A')}")

ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches (search wider range)
        for j in range(max(0, idx-60), min(len(ground_truth), idx+60)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}] (offset: {idx - j})")
                break
        else:
            print(f"  -> No match found in range [{max(0,idx-60)}, {min(len(ground_truth), idx+60)})")

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

```plaintext
d presentation_idx_to_pts[63] should be 559 (the 64th smallest PTS, 0-indexed).

Let me check our rust mapping. The presentation_idx_to_pts should have:- index 56 = 489 (keyframe 56's PTS)- index 63 = 559 (packet 64's PTS)

But we're searching for PTS=559 and finding ground_truth[56]. This is 7 positions off.

Oh wait, I see the issue! Looking at the sorted output:- rank 6: packet 7 (PTS=-0.0167)- rank 7: packet 6 (PTS=0.0000)

The first 7 frames (ranks 0-6) are packets 0,1,2,3,4,5,7. But rank 7 is packet 6!

So when sorted by PTS:
- Presentation index 0-5: packets 0-5 (in order)- Presentation index 6: packet 7 (PTS=-0.0167)- Presentation index 7: packet 6 (PTS=0.0000)

Now looking at position 56 vs 63:
- Position 56: packet 56, PTS=0.8150 (489)- Position 63: packet 64, PTS=0.9317 (559)

So presentation index 63 should have PTS=559 (from packet 64).


I'm puzzled by the discrepancy. Ground_truth[56] seems to match what we'd expect for frame 63, containing packet 56's content with PTS=489. This contradicts our expected mapping of presentation indices to PTS values. The iterator's output doesn't align with the sorted PTS order I've been analyzing, creating a confusing mismatch between expected and actual frame positions.

My suspicion grows that the Python iterator might not respect strict PTS sorting. Instead, it could be returning frames in the sequence they were initially received, which would explain the unexpected alignment of frame indices and packet timestamps.

I'll investigate the decode mechanism to understand why our frame retrieval doesn't match the anticipated PTS-based ordering.

The packet sending and frame decoding process suggests potential complexity with B-frame buffering. The decoder might require multiple packet inputs before successfully generating a frame, which could disrupt the expected sequential output.

Examining the `decode_frames()` implementation reveals nuanced frame retrieval dynamics that might explain the observed inconsistencies in frame ordering.

The decoding mechanism appears sensitive to packet timing and presentation timestamp (PTS) sequencing, with frames potentially being buffered and released non-linearly based on internal decoder logic.

Investigating the initial frame sequence shows intricate interactions between packet submission and frame generation, particularly around I-frame and B-frame processing.

The GOP (Group of Pictures) structure reveals a predictable keyframe pattern, with I-frames at specific intervals and intermediate frames potentially requiring complex dependency resolution before output.

Codec-specific buffering means frame emission isn't strictly sequential, creating potential variations between packet submission order and actual frame rendering.

Investigating the frame offsets reveals an intriguing relationship: the 7-frame discrepancy precisely corresponds to frames with negative presentation timestamps, suggesting a nuanced frame handling mechanism that strategically manages pre-zero timestamp frames.

The current approach requires a careful reconsideration of frame iteration strategies, particularly when seeking isn't straightforward for B-frame encoded videos.

I'll focus on developing a targeted solution that accounts for the systematic offset created by negative PTS frames, potentially by implementing a preprocessing step that adjusts frame indexing to align with ground truth expectations.

The key observation is the consistent 7-frame displacement, which suggests a predictable transformation can resolve the alignment discrepancy. By shifting the frame references to compensate for the negative PTS frames, we can potentially realign the decoder's output with the expected ground truth sequence.

A pragmatic approach would involve creating a mapping that explicitly accounts for this offset, ensuring frame correspondence without complex algorithmic restructuring.
```

由于时间复杂性，让我采取一个更简单的方法。问题是有 7 帧的 PTS 是负数（小于 0），这导致了偏移。让我计算这个偏移并调整：

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
    /// Mapping from presentation index to PTS (sorted PTS list)
    presentation_idx_to_pts: Vec<i64>,
    /// Number of frames with negative PTS (affects frame indexing offset)
    negative_pts_count: usize,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        // Build presentation index to PTS mapping (sorted PTS list)
        let presentation_idx_to_pts: Vec<i64> = pts_sorted
            .iter()
            .map(|(pts, _)| *pts)
            .collect();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values",
            frame_count,
            pts_to_presentation_idx.len()
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
            presentation_idx_to_pts,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    /// Get the target PTS for a presentation index (sorted by PTS)
    pub fn get_pts_for_presentation_idx(&self, presentation_idx: usize) -> Option<i64> {
        self.presentation_idx_to_pts.get(presentation_idx).copied()
    }
    /// Get the PTS for a decode order index (packet order)
    /// This is used when the decoder outputs frames in decode order without reordering
    pub fn get_pts_for_decode_idx(&self, decode_idx: usize) -> Option<i64> {
        self.frame_times.get(&decode_idx).map(|ft| ft.pts)
    }
    /// Get presentation index from decode index (packet index)
    /// In B-frame videos, a packet at decode_idx K will produce a frame
    /// at presentation index M (where M is determined by the frame's PTS relative to others)
    pub fn get_presentation_idx_from_decode(&self, decode_idx: usize) -> Option<usize> {
        let pts = self.frame_times.get(&decode_idx)?.pts;
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

这个问题太复杂了。让我尝试一个更直接的方法：使用**迭代器输出的 PTS 顺序**来匹配，而不是假设的 presentation order。实际上，iterator 可能按**包顺序**输出，而不是 PTS 排序顺序。

让我直接使用包顺序 PTS 并通过正确计数来匹配帧：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a presentation order index (iterator output order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in presentation order (same as iterator output order)
        // Convert to decode index to find the appropriate keyframe
        let target_decode_idx = self
            .stream_info
            .get_decode_idx_from_presentation(frame_index)
            .unwrap_or(frame_index);

        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&target_decode_idx);

        debug!(
            "    - Target presentation idx: {}, decode idx: {}, keyframe decode idx: {}",
            frame_index, target_decode_idx, key_decode_idx
        );
        debug!(
            "    - curr_frame (presentation): {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // We can only skip if we're already at or past the keyframe for the target
        let can_skip_forward = (frame_index >= self.curr_frame)
            && (target_decode_idx >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_frame);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            // For backwards seeking, first go to start then seek forward
            // This is more reliable than direct backward seeking
            if key_decode_idx < self.curr_dec_idx {
                debug!("Backward seek detected, going to start first");
                self.seek_to_start()?;
            }
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            // After seeking, we need to decode until we find the frame with the target PTS
            // The keyframe's presentation index might be different from its decode index
            let key_presentation_idx = self
                .stream_info
                .get_presentation_idx_from_decode(key_decode_idx)
                .unwrap_or(key_decode_idx);
            self.curr_frame = key_presentation_idx;
            let num_skip = frame_index.saturating_sub(key_presentation_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (iterator output order index)
    /// The iterator outputs frames in packet order (decode order), so we match
    /// by the PTS of the packet at frame_index.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the expected PTS for the target frame index in PACKET order
        // The iterator processes packets in order and outputs frames as decoder produces them
        // For B-frame videos, this is still packet order, not sorted PTS order
        let target_pts = self.stream_info.get_pts_for_decode_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, target={}, target_pts={:?}",
                pts, frame_index, target_pts
            );

            // Match by PTS from packet order
            if let Some(expected_pts) = target_pts {
                if pts == expected_pts {
                    debug!(
                        "Found target frame {} with PTS {}",
                        frame_index, pts
                    );
                    rgb_frame = self.decoder.process_frame(&decoded);
                }
            }

            self.curr_dec_idx += 1;
            self.curr_frame += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a packet/decode order index (iterator output order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in packet/decode order (same as iterator output order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // We can only skip if we're already at or past the keyframe for the target
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            // For backwards seeking, first go to start then seek forward
            if key_decode_idx < self.curr_dec_idx {
                debug!("Backward seek detected, going to start first");
                self.seek_to_start()?;
            }
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            self.curr_frame = key_decode_idx;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (iterator output order index)
    /// The iterator outputs frames in packet order (decode order), so we match
    /// by the PTS of the packet at frame_index.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Get the expected PTS for the target frame index in PACKET order
        // The iterator processes packets in order and outputs frames as decoder produces them
        // For B-frame videos, this is still packet order, not sorted PTS order
        let target_pts = self.stream_info.get_pts_for_decode_idx(*frame_index);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, target={}, target_pts={:?}",
                pts, frame_index, target_pts
            );

            // Match by PTS from packet order
            if let Some(expected_pts) = target_pts {
                if pts == expected_pts {
                    debug!(
                        "Found target frame {} with PTS {}",
                        frame_index, pts
                    );
                    rgb_frame = self.decoder.process_frame(&decoded);
                }
            }

            self.curr_dec_idx += 1;
            self.curr_frame += 1;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
    /// Mapping from presentation index to PTS (sorted PTS list)
    presentation_idx_to_pts: Vec<i64>,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        // Build presentation index to PTS mapping (sorted PTS list)
        let presentation_idx_to_pts: Vec<i64> = pts_sorted
            .iter()
            .map(|(pts, _)| *pts)
            .collect();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values",
            frame_count,
            pts_to_presentation_idx.len()
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
            presentation_idx_to_pts,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    /// Get the target PTS for a presentation index (sorted by PTS)
    pub fn get_pts_for_presentation_idx(&self, presentation_idx: usize) -> Option<i64> {
        self.presentation_idx_to_pts.get(presentation_idx).copied()
    }
    /// Get the PTS for a decode order index (packet order)
    /// This is used when the decoder outputs frames in decode order without reordering
    pub fn get_pts_for_decode_idx(&self, decode_idx: usize) -> Option<i64> {
        self.frame_times.get(&decode_idx).map(|ft| ft.pts)
    }
    /// Get presentation index from decode index (packet index)
    /// In B-frame videos, a packet at decode_idx K will produce a frame
    /// at presentation index M (where M is determined by the frame's PTS relative to others)
    pub fn get_presentation_idx_from_decode(&self, decode_idx: usize) -> Option<usize> {
        let pts = self.frame_times.get(&decode_idx)?.pts;
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

```rust
if with_fallback
    && ((num_zero_pts > 1)
        || (first_key.dur() <= &0)
        || (first_key.dts() < &0)
        || start_time > 0)
```

问题太复杂了。让我尝试最简单的方法 - 从 keyframe 位置开始计数解码帧：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a packet/decode order index (iterator output order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in packet/decode order (same as iterator output order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // We can only skip if we're already at or past the keyframe for the target
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            // For backwards seeking, first go to start then seek forward
            if key_decode_idx < self.curr_dec_idx {
                debug!("Backward seek detected, going to start first");
                self.seek_to_start()?;
            }
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            self.curr_frame = key_decode_idx;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator output)
    /// After seeking to keyframe K, the N-th decoded frame is at position K+N-1.
    /// We count frames to find the target position.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={}",
                pts, self.curr_dec_idx, frame_index
            );

            // Match by position count (curr_dec_idx)
            // After seek to keyframe K, curr_dec_idx starts at K
            // Each decoded frame increments curr_dec_idx
            if self.curr_dec_idx == *frame_index {
                debug!(
                    "Found target frame {} at position {}",
                    frame_index, self.curr_dec_idx
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            self.curr_dec_idx += 1;
            self.curr_frame = self.curr_dec_idx;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with keyframe position directly to verify basic seek works
batch_indexs = [56, 57, 58, 63]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
# Get PTS info first
pts_list = vr_rs.get_pts()
print(f"Total frames: {len(vr_rs)}")
print(f"First 20 PTS values: {pts_list[:20]}")

# Check PTS order (are they sorted?)
is_sorted = all(pts_list[i] <= pts_list[i+1] for i in range(len(pts_list)-1))
print(f"PTS values are sorted: {is_sorted}")

# Check what the 63rd frame's PTS actually is in ground_truth order (packet order)
print(f"PTS of ground_truth[63] (packet order): {pts_list[63]:.4f} = {pts_list[63] * 600:.0f} timestamp units")
print(f"PTS of ground_truth[56] (packet order): {pts_list[56]:.4f} = {pts_list[56] * 600:.0f} timestamp units")

# Print PTS sequence for frames 55-70
print("\nPTS sequence for frames 55-70 (packet order):")
for i in range(55, 71):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f} = {pts_list[i] * 600:.0f} ts")

# Check if iterator output is in PTS sorted order by examining the actual frames
# This is the key test: is ground_truth[N] the frame with (N+1)th smallest PTS?
sorted_pts = sorted(enumerate(pts_list), key=lambda x: x[1])
print("\nFirst 10 frames in sorted PTS order:")
for rank, (packet_idx, pts_val) in enumerate(sorted_pts[:10]):
    print(f"  rank {rank}: packet {packet_idx}, PTS = {pts_val:.4f}")

# Check which packet corresponds to ground_truth[63]
# If iterator outputs in PTS order, ground_truth[63] should match packet at sorted_pts[63]
print(f"\nSorted PTS position 56: packet {sorted_pts[56][0]}, PTS = {sorted_pts[56][1]:.4f}")
print(f"Sorted PTS position 63: packet {sorted_pts[63][0]}, PTS = {sorted_pts[63][1]:.4f}")

# Check PTS around keyframe 56
print(f"PTS around keyframe 56 (packets 50-65):")
for i in range(50, 66):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f}")

# Check video info
info = vr_rs.get_info()
print(f"\nVideo info:")
print(f"  start_time: {info.get('start_time', 'N/A')}")
print(f"  time_base: {info.get('time_base', 'N/A')}")

ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches (search wider range)
        for j in range(max(0, idx-60), min(len(ground_truth), idx+60)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}] (offset: {idx - j})")
                break
        else:
            print(f"  -> No match found in range [{max(0,idx-60)}, {min(len(ground_truth), idx+60)})")

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

偏移稳定在 7。这说明 seek 后的输出与从头迭代相比有固定偏移。让我尝试调整：

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Test with keyframe position directly to verify basic seek works
batch_indexs = [56, 57, 58, 63]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
# Get PTS info first
pts_list = vr_rs.get_pts()
print(f"Total frames: {len(vr_rs)}")
print(f"First 20 PTS values: {pts_list[:20]}")

# Check PTS order (are they sorted?)
is_sorted = all(pts_list[i] <= pts_list[i+1] for i in range(len(pts_list)-1))
print(f"PTS values are sorted: {is_sorted}")

# Check what the 63rd frame's PTS actually is in ground_truth order (packet order)
print(f"PTS of ground_truth[63] (packet order): {pts_list[63]:.4f} = {pts_list[63] * 600:.0f} timestamp units")
print(f"PTS of ground_truth[56] (packet order): {pts_list[56]:.4f} = {pts_list[56] * 600:.0f} timestamp units")

# Print PTS sequence for frames 55-70
print("\nPTS sequence for frames 55-70 (packet order):")
for i in range(55, 71):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f} = {pts_list[i] * 600:.0f} ts")

# Check if iterator output is in PTS sorted order by examining the actual frames
# This is the key test: is ground_truth[N] the frame with (N+1)th smallest PTS?
sorted_pts = sorted(enumerate(pts_list), key=lambda x: x[1])
print("\nFirst 10 frames in sorted PTS order:")
for rank, (packet_idx, pts_val) in enumerate(sorted_pts[:10]):
    print(f"  rank {rank}: packet {packet_idx}, PTS = {pts_val:.4f}")

# Count frames with negative PTS
negative_pts_count = sum(1 for pts in pts_list if pts < 0)
print(f"\nNumber of frames with negative PTS: {negative_pts_count}")

# Check which packet corresponds to ground_truth[63]
# If iterator outputs in PTS order, ground_truth[63] should match packet at sorted_pts[63]
print(f"\nSorted PTS position 56: packet {sorted_pts[56][0]}, PTS = {sorted_pts[56][1]:.4f}")
print(f"Sorted PTS position 63: packet {sorted_pts[63][0]}, PTS = {sorted_pts[63][1]:.4f}")

# Check PTS around keyframe 56
print(f"PTS around keyframe 56 (packets 50-65):")
for i in range(50, 66):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f}")

# Check video info
info = vr_rs.get_info()
print(f"\nVideo info:")
print(f"  start_time: {info.get('start_time', 'N/A')}")
print(f"  time_base: {info.get('time_base', 'N/A')}")

ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches (search wider range)
        for j in range(max(0, idx-60), min(len(ground_truth), idx+60)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}] (offset: {idx - j})")
                break
        else:
            print(f"  -> No match found in range [{max(0,idx-60)}, {min(len(ground_truth), idx+60)})")

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

确认了！负 PTS 帧数量（7）正好等于偏移量。这是 FFmpeg 处理负 PTS 帧的方式导致的。

让我在 Rust 代码中计算并应用这个偏移：

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
    /// Mapping from presentation index to PTS (sorted PTS list)
    presentation_idx_to_pts: Vec<i64>,
    /// Number of frames with negative PTS (used for seek offset correction)
    negative_pts_offset: usize,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        // Build presentation index to PTS mapping (sorted PTS list)
        let presentation_idx_to_pts: Vec<i64> = pts_sorted
            .iter()
            .map(|(pts, _)| *pts)
            .collect();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values",
            frame_count,
            pts_to_presentation_idx.len()
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
            presentation_idx_to_pts,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    /// Get the target PTS for a presentation index (sorted by PTS)
    pub fn get_pts_for_presentation_idx(&self, presentation_idx: usize) -> Option<i64> {
        self.presentation_idx_to_pts.get(presentation_idx).copied()
    }
    /// Get the PTS for a decode order index (packet order)
    /// This is used when the decoder outputs frames in decode order without reordering
    pub fn get_pts_for_decode_idx(&self, decode_idx: usize) -> Option<i64> {
        self.frame_times.get(&decode_idx).map(|ft| ft.pts)
    }
    /// Get presentation index from decode index (packet index)
    /// In B-frame videos, a packet at decode_idx K will produce a frame
    /// at presentation index M (where M is determined by the frame's PTS relative to others)
    pub fn get_presentation_idx_from_decode(&self, decode_idx: usize) -> Option<usize> {
        let pts = self.frame_times.get(&decode_idx)?.pts;
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
    /// Mapping from presentation index to PTS (sorted PTS list)
    presentation_idx_to_pts: Vec<i64>,
    /// Number of frames with negative PTS (used for seek offset correction)
    negative_pts_offset: usize,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        // Build presentation index to PTS mapping (sorted PTS list)
        let presentation_idx_to_pts: Vec<i64> = pts_sorted
            .iter()
            .map(|(pts, _)| *pts)
            .collect();

        // Count frames with negative PTS (these cause seek offset issues)
        let negative_pts_offset = frame_times
            .values()
            .filter(|ft| ft.pts < 0)
            .count();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values, {} negative PTS frames",
            frame_count,
            pts_to_presentation_idx.len(),
            negative_pts_offset
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
            presentation_idx_to_pts,
            negative_pts_offset,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    /// Get the target PTS for a presentation index (sorted by PTS)
    pub fn get_pts_for_presentation_idx(&self, presentation_idx: usize) -> Option<i64> {
        self.presentation_idx_to_pts.get(presentation_idx).copied()
    }
    /// Get the PTS for a decode order index (packet order)
    /// This is used when the decoder outputs frames in decode order without reordering
    pub fn get_pts_for_decode_idx(&self, decode_idx: usize) -> Option<i64> {
        self.frame_times.get(&decode_idx).map(|ft| ft.pts)
    }
    /// Get presentation index from decode index (packet index)
    /// In B-frame videos, a packet at decode_idx K will produce a frame
    /// at presentation index M (where M is determined by the frame's PTS relative to others)
    pub fn get_presentation_idx_from_decode(&self, decode_idx: usize) -> Option<usize> {
        let pts = self.frame_times.get(&decode_idx)?.pts;
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

```rust
use ffmpeg::codec::packet::side_data::Type as SideDataType;
use ffmpeg::ffi::av_display_rotation_get;

use ffmpeg::util::rational::Rational;
use ffmpeg::Stream;
use ffmpeg_next as ffmpeg;
use std::collections::{BTreeMap, HashMap};
use log::debug;

pub struct VideoParams {
    duration: f64,
    start_time: i64,
    time_base: f64,
    time_base_rational: Rational,
    pub rotation: isize,
}

/// Timing info for key frames
#[derive(Debug)]
pub struct FrameTime {
    pts: i64,
    dur: i64,
    dts: i64,
}

impl FrameTime {
    pub fn new(pts: i64, dur: i64, dts: i64) -> Self {
        FrameTime { pts, dur, dts }
    }
    pub fn pts(&self) -> &i64 {
        &self.pts
    }
    pub fn dts(&self) -> &i64 {
        &self.dts
    }
    pub fn dur(&self) -> &i64 {
        &self.dur
    }
}

/// Info gathered from iterating over the video stream
pub struct StreamInfo {
    frame_count: usize,
    key_frames: Vec<usize>,
    frame_times: BTreeMap<usize, FrameTime>,
    /// Mapping from PTS to presentation order index (display order)
    /// This is needed for B-frame videos where decode order != display order
    pts_to_presentation_idx: HashMap<i64, usize>,
    /// Mapping from presentation index to decode index
    presentation_to_decode_idx: HashMap<usize, usize>,
    /// Mapping from presentation index to PTS (sorted PTS list)
    presentation_idx_to_pts: Vec<i64>,
    /// Number of frames with negative PTS (used for seek offset correction)
    negative_pts_offset: usize,
}

impl StreamInfo {
    pub fn new(
        frame_count: usize,
        key_frames: Vec<usize>,
        frame_times: BTreeMap<usize, FrameTime>,
    ) -> Self {
        // Build mapping from PTS to presentation order (display order)
        // For B-frame videos, we need to sort by PTS to get the correct display order
        let mut pts_sorted: Vec<(i64, usize)> = frame_times
            .iter()
            .map(|(decode_idx, ft)| (ft.pts, *decode_idx))
            .collect();
        pts_sorted.sort_by_key(|(pts, _)| *pts);

        // Now pts_sorted[presentation_idx] = (pts, decode_idx)
        let pts_to_presentation_idx: HashMap<i64, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (pts, _))| (*pts, presentation_idx))
            .collect();

        let presentation_to_decode_idx: HashMap<usize, usize> = pts_sorted
            .iter()
            .enumerate()
            .map(|(presentation_idx, (_, decode_idx))| (presentation_idx, *decode_idx))
            .collect();

        // Build presentation index to PTS mapping (sorted PTS list)
        let presentation_idx_to_pts: Vec<i64> = pts_sorted
            .iter()
            .map(|(pts, _)| *pts)
            .collect();

        // Count frames with negative PTS (these cause seek offset issues)
        let negative_pts_offset = frame_times
            .values()
            .filter(|ft| ft.pts < 0)
            .count();

        debug!(
            "Built presentation order mapping: {} frames, {} unique PTS values, {} negative PTS frames",
            frame_count,
            pts_to_presentation_idx.len(),
            negative_pts_offset
        );
        
        // Debug: print first 20 mappings
        for i in 0..20.min(pts_sorted.len()) {
            let (pts, decode_idx) = pts_sorted[i];
            debug!(
                "Presentation idx {} -> PTS {} -> decode idx {}",
                i, pts, decode_idx
            );
        }

        StreamInfo {
            frame_count,
            key_frames,
            frame_times,
            pts_to_presentation_idx,
            presentation_to_decode_idx,
            presentation_idx_to_pts,
            negative_pts_offset,
        }
    }
    pub fn frame_count(&self) -> &usize {
        &self.frame_count
    }
    pub fn key_frames(&self) -> &Vec<usize> {
        &self.key_frames
    }
    pub fn frame_times(&self) -> &BTreeMap<usize, FrameTime> {
        &self.frame_times
    }
    /// Get presentation order index from PTS value
    /// This returns the display order index for a given PTS
    pub fn get_presentation_idx_from_pts(&self, pts: i64) -> Option<usize> {
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get decode order index from presentation order index
    pub fn get_decode_idx_from_presentation(&self, presentation_idx: usize) -> Option<usize> {
        self.presentation_to_decode_idx.get(&presentation_idx).copied()
    }
    /// Get the target PTS for a presentation index (sorted by PTS)
    pub fn get_pts_for_presentation_idx(&self, presentation_idx: usize) -> Option<i64> {
        self.presentation_idx_to_pts.get(presentation_idx).copied()
    }
    /// Get the PTS for a decode order index (packet order)
    /// This is used when the decoder outputs frames in decode order without reordering
    pub fn get_pts_for_decode_idx(&self, decode_idx: usize) -> Option<i64> {
        self.frame_times.get(&decode_idx).map(|ft| ft.pts)
    }
    /// Get presentation index from decode index (packet index)
    /// In B-frame videos, a packet at decode_idx K will produce a frame
    /// at presentation index M (where M is determined by the frame's PTS relative to others)
    pub fn get_presentation_idx_from_decode(&self, decode_idx: usize) -> Option<usize> {
        let pts = self.frame_times.get(&decode_idx)?.pts;
        self.pts_to_presentation_idx.get(&pts).copied()
    }
    /// Get the number of frames with negative PTS (seek offset correction)
    pub fn negative_pts_offset(&self) -> usize {
        self.negative_pts_offset
    }
    pub fn get_all_pts(&self, time_base: f64) -> Vec<f64> {
        self.frame_times
            .values()
            .map(|v| v.pts as f64 * time_base)
            .collect::<Vec<_>>()
    }
    pub fn get_pts(&self, indices: &[usize], time_base: f64) -> Vec<f64> {
        indices
            .iter()
            .map(|i| match self.frame_times.get(i) {
                None => -1.,
                Some(ft) => ft.pts as f64 * time_base,
            })
            .collect::<Vec<_>>()
    }
}

fn get_rotation_angle(input: &Stream) -> isize {
    for sd in input.side_data() {
        if sd.kind() == SideDataType::DisplayMatrix {
            let matrix = sd.data();
            let theta = unsafe { av_display_rotation_get(matrix.as_ptr() as *const _) };
            if theta.is_nan() {
                return 0_isize;
            } else {
                return theta.round() as isize;
            }
        }
    }
    0_isize
}

pub fn extract_video_params(input: &Stream) -> VideoParams {
    VideoParams {
        duration: input.duration() as f64 * f64::from(input.time_base()),
        start_time: input.start_time(),
        time_base: f64::from(input.time_base()),
        time_base_rational: input.time_base(),
        rotation: get_rotation_angle(input),
    }
}

pub fn collect_video_metadata(
    video: &ffmpeg::decoder::Video,
    params: &VideoParams,
    fps: &f64,
) -> HashMap<&'static str, String> {
    let mut info = HashMap::new();

    let fps_rational = video.frame_rate().unwrap_or(Rational(0, 1));
    info.insert("fps_rational", fps_rational.to_string());
    info.insert("fps", format!("{fps}"));
    info.insert("start_time", params.start_time.to_string());
    info.insert("time_base", params.time_base.to_string());
    info.insert("time_base_rational", params.time_base_rational.to_string());
    info.insert("duration", params.duration.to_string());
    info.insert("codec_id", format!("{:?}", video.codec().unwrap().id()));
    info.insert("height", video.height().to_string());
    info.insert("width", video.width().to_string());
    info.insert("bit_rate", video.bit_rate().to_string());
    info.insert("vid_format", format!("{:?}", video.format()));
    info.insert("aspect_ratio", format!("{:?}", video.aspect_ratio()));
    let color_space = format!("{:?}", video.color_space());
    info.insert("color_space", color_space.to_uppercase());
    let color_range = format!("{:?}", video.color_range());
    info.insert("color_range", color_range.to_uppercase());
    info.insert("color_primaries", format!("{:?}", video.color_primaries()));
    info.insert(
        "color_xfer_charac",
        format!("{:?}", video.color_transfer_characteristic()),
    );
    info.insert("chroma_location", format!("{:?}", video.chroma_location()));
    info.insert("vid_ref", video.references().to_string());
    info.insert("intra_dc_precision", video.intra_dc_precision().to_string());
    info.insert("has_b_frames", format!("{}", video.has_b_frames()));
    info.insert("rotation", format!("{}", params.rotation));
    info
}

pub fn get_frame_count(
    ictx: &mut ffmpeg::format::context::Input,
    stream_index: &usize,
) -> Result<StreamInfo, ffmpeg::Error> {
    // keep track of decoding index, ie frame count
    let mut didx = 0_usize;
    let mut key_frames = Vec::new();
    let mut frame_times = BTreeMap::new();

    for (stream, packet) in ictx.packets() {
        if &stream.index() == stream_index {
            let pts = packet.pts().unwrap_or(0);
            let dur = packet.duration();
            let dts = packet.dts().unwrap_or(0);
            frame_times.insert(didx, FrameTime::new(pts, dur, dts));
            if packet.is_key() {
                key_frames.push(didx);
            }
            didx += 1;
        }
    }

    // Seek back to the begining of the stream
    ictx.seek(0, ..10)?;
    Ok(StreamInfo::new(didx, key_frames, frame_times))
}

/// Get the resized dimension of a frame, keep the aspect ratio.
/// Resize the shorter side of the frame, and the other side accordingly
/// so that the resizing operation is minimal. Returns a resizing dimension only
/// if the shorter side of the frame is bigger than resize_shorter_side_to.
/// * height (f64): Height of the frame
/// * width (f64): Width of the frame
/// * resize_shorter_side_to (Option<f64>): Resize the shorter side of the frame to this value.
/// * resize_longer_side_to (Option<f64>): Resize the longer side of the frame to this value.
///
/// Returns: Option<(u32, u32)>: Option of the resized height and width
pub fn get_resized_dim(
    height: f64,
    width: f64,
    resize_shorter_side_to: Option<f64>,
    resize_longer_side_to: Option<f64>,
) -> (u32, u32) {
    let mut respect_aspect_ratio = true;
    // early return when nothing to do
    if resize_shorter_side_to.is_none() && resize_longer_side_to.is_none() {
        return (height as u32, width as u32);
    } else if resize_shorter_side_to.is_some() && resize_longer_side_to.is_some() {
        respect_aspect_ratio = false;
    }

    let mut shorter_is_height: bool = true;
    let new_height: f64;
    let new_width: f64;

    if width < height {
        shorter_is_height = false;
    }

    if !respect_aspect_ratio {
        // in this case both values are Some so it should be safe to unwrap
        if shorter_is_height {
            new_height = resize_shorter_side_to.unwrap();
            new_width = resize_longer_side_to.unwrap();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = resize_shorter_side_to.unwrap();
        }
        return (new_height as u32, new_width as u32);
    }

    // Only case remaining is one is None and the other is Some
    if let Some(res_short_side) = resize_shorter_side_to {
        if shorter_is_height {
            new_height = res_short_side;
            new_width = (width * new_height / height).round();
        } else {
            new_width = res_short_side;
            new_height = (height * new_width / width).round();
        }
    } else {
        // resize_longer_side_to can only be Some at this point
        if shorter_is_height {
            new_width = resize_longer_side_to.unwrap();
            new_height = (height * new_width / width).round();
        } else {
            new_height = resize_longer_side_to.unwrap();
            new_width = (width * new_height / height).round();
        }
    }
    (new_height as u32, new_width as u32)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_no_resize_needed() {
        let height = 720.0;
        let width = 1280.0;
        let result = get_resized_dim(height, width, None, None);

        assert_eq!(result, (720, 1280));
    }

    #[test]
    fn test_resize_shorter_side_height() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 480);
        assert_eq!(result.1, 853);
    }

    #[test]
    fn test_resize_shorter_side_width() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 480.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.1, 480);
        assert_eq!(result.0, 720);
    }

    #[test]
    fn test_resize_longer_side_height() {
        let height = 1080.0;
        let width = 720.0;
        let resize_to = 720.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.0, 720);
        assert_eq!(result.1, 480);
    }

    #[test]
    fn test_resize_longer_side_width() {
        let height = 720.0;
        let width = 1280.0;
        let resize_to = 854.0;

        let result = get_resized_dim(height, width, None, Some(resize_to));

        assert_eq!(result.1, 854);
        assert_eq!(result.0, 480);
    }

    #[test]
    fn test_both_dimensions_specified() {
        let height = 720.0;
        let width = 1280.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should ignore aspect ratio and use the provided dimensions
        assert_eq!(result, (480, 800));
    }

    #[test]
    fn test_both_dimensions_specified_width_smaller() {
        let height = 1080.0;
        let width = 720.0;

        let result = get_resized_dim(height, width, Some(480.0), Some(800.0));

        // Should set width to shorter_side value and height to longer_side value
        assert_eq!(result, (800, 480));
    }

    #[test]
    fn test_resize_equal_dimensions() {
        let height = 1000.0;
        let width = 1000.0;

        let result = get_resized_dim(height, width, Some(500.0), None);

        assert_eq!(result, (500, 500));
    }

    #[test]
    fn test_rounding_behavior() {
        let height = 723.0;
        let width = 1283.0;
        let resize_to = 483.0;

        let result = get_resized_dim(height, width, Some(resize_to), None);

        assert_eq!(result.0, 483);
        // 1283 * (483/723) = 857.1 which should round to 857
        assert_eq!(result.1, 857);
    }
}

```

现在更新 `get_frame` 来应用偏移校正：

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a packet/decode order index (iterator output order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // frame_index is in packet/decode order (same as iterator output order)
        // Find the keyframe before this decode index
        let key_decode_idx = self.locate_keyframes(&frame_index);

        debug!(
            "    - Target decode idx: {}, keyframe decode idx: {}",
            frame_index, key_decode_idx
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // We can only skip if we're already at or past the keyframe for the target
        let can_skip_forward = (frame_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until frame_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = frame_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            // For backwards seeking, first go to start then seek forward
            if key_decode_idx < self.curr_dec_idx {
                debug!("Backward seek detected, going to start first");
                self.seek_to_start()?;
            }
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            self.curr_frame = key_decode_idx;
            let num_skip = frame_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator output)
    /// After seeking to keyframe K, the N-th decoded frame is at position K+N-1.
    /// We count frames to find the target position, with offset correction for negative PTS frames.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Apply offset correction for videos with negative PTS frames
        // After seeking, frames are offset by the number of negative PTS frames
        let offset = self.stream_info.negative_pts_offset();
        let adjusted_target = frame_index.saturating_add(offset);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={} (adjusted={})",
                pts, self.curr_dec_idx, frame_index, adjusted_target
            );

            // Match by position count with offset adjustment
            // After seek to keyframe K, curr_dec_idx starts at K
            // Each decoded frame increments curr_dec_idx
            // We need to match at adjusted_target to account for negative PTS frames
            if self.curr_dec_idx == adjusted_target {
                debug!(
                    "Found target frame {} (adjusted {}) at position {}",
                    frame_index, adjusted_target, self.curr_dec_idx
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            self.curr_dec_idx += 1;
            self.curr_frame = self.curr_dec_idx;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

```rust
use ffmpeg::codec::threading;
use ffmpeg::ffi::*;
use ffmpeg::format::input;
use ffmpeg::media::Type;
use ffmpeg::util::frame::video::Video;
use ffmpeg_next as ffmpeg;
use log::debug;
use std::collections::{HashMap, HashSet};
use std::thread;
use std::path::Path;

use crate::convert::{convert_nv12_to_ndarray_rgb24, convert_yuv_to_ndarray_rgb24};
use crate::decoder::{DecoderConfig, VideoDecoder, VideoReducer};
use crate::filter::{create_filter_spec, create_filters, FilterConfig};
use crate::hwaccel::{HardwareAccelerationContext, HardwareAccelerationDeviceType};
use crate::info::{
    collect_video_metadata, extract_video_params, get_frame_count, get_resized_dim, StreamInfo,
};
use crate::utils::{insert_frame, FrameArray, VideoArray, HWACCEL_PIXEL_FORMAT};
use ndarray::{s, Array, Array4, ArrayViewMut3};
use tokio::task;

pub fn get_init_context(
    filename: &String,
) -> Result<(ffmpeg::format::context::Input, usize), ffmpeg::Error> {
    let input_file = Path::new(filename);

    // Initialize the FFmpeg library
    ffmpeg::init()?;

    // Open the input file
    let ictx = input(&input_file)?;
    let stream_index = ictx
        .streams()
        .best(Type::Video)
        .ok_or(ffmpeg::Error::StreamNotFound)?
        .index();
    Ok((ictx, stream_index))
}

fn setup_decoder_context(
    input: &ffmpeg::Stream,
    threads: usize,
    hwaccel_device_type: Option<HardwareAccelerationDeviceType>,
) -> Result<
    (
        ffmpeg::codec::context::Context,
        Option<HardwareAccelerationContext>,
    ),
    ffmpeg::Error,
> {
    let mut context = ffmpeg::codec::context::Context::from_parameters(input.parameters())?;

    let hwaccel_context = match hwaccel_device_type {
        Some(device_type) => Some(HardwareAccelerationContext::new(&mut context, device_type)?),
        None => None,
    };
    context.set_threading(threading::Config {
        kind: threading::Type::Frame,
        count: threads,
        #[cfg(feature = "ffmpeg_5")]
        safe: true,
    });
    Ok((context, hwaccel_context))
}

/// Struct responsible for reading the stream and getting the metadata
pub struct VideoReader {
    ictx: ffmpeg::format::context::Input,
    stream_index: usize,
    stream_info: StreamInfo,
    curr_frame: usize,
    curr_dec_idx: usize,
    n_fails: usize,
    decoder: VideoDecoder,
    draining: bool,
}

unsafe impl Send for VideoReader {}

impl VideoReader {
    pub fn decoder(&self) -> &VideoDecoder {
        &self.decoder
    }
    pub fn stream_info(&self) -> &StreamInfo {
        &self.stream_info
    }
    /// Create a new VideoReader instance
    /// * `filename` - Path to the video file.
    /// * `decoder_config` - Config for the decoder see: [`DecoderConfig`]
    ///
    /// Returns: a VideoReader instance.
    pub fn new(
        filename: String,
        decoder_config: DecoderConfig,
    ) -> Result<VideoReader, ffmpeg::Error> {
        let (mut ictx, stream_index) = get_init_context(&filename)?;
        let stream_info = get_frame_count(&mut ictx, &stream_index)?;
        let decoder = Self::get_decoder(&ictx, decoder_config)?;
        debug!("frame_count: {}", stream_info.frame_count());
        debug!("key frames: {:?}", stream_info.key_frames());
        Ok(VideoReader {
            ictx,
            stream_index,
            stream_info,
            curr_frame: 0,
            curr_dec_idx: 0,
            n_fails: 0,
            decoder,
            draining: false,
        })
    }

    pub fn get_decoder(
        ictx: &ffmpeg::format::context::Input,
        config: DecoderConfig,
    ) -> Result<VideoDecoder, ffmpeg::Error> {
        let input = ictx
            .streams()
            .best(Type::Video)
            .ok_or(ffmpeg::Error::StreamNotFound)?;

        let fps = f64::from(input.avg_frame_rate());
        let video_params = extract_video_params(&input);

        let (decoder_context, hwaccel_context) =
            setup_decoder_context(&input, config.threads(), config.hwaccel())?;

        let mut video = decoder_context.decoder().video()?;
        let video_info = collect_video_metadata(&video, &video_params, &fps);

        let (orig_h, orig_w, orig_fmt) = (video.height(), video.width(), video.format());

        let (mut height, mut width) = get_resized_dim(
            orig_h as f64,
            orig_w as f64,
            config.resize_shorter_side(),
            config.resize_longer_side(),
        );

        let is_hwaccel = hwaccel_context.is_some();
        let (filter_spec, hw_format) = create_filter_spec(
            width,
            height,
            &mut video,
            config.ff_filter(),
            hwaccel_context,
            HWACCEL_PIXEL_FORMAT,
            video_params.rotation,
        )?;

        debug!("Filter spec: {}", filter_spec);
        let filter_cfg = FilterConfig::new(
            orig_h,
            orig_w,
            orig_fmt,
            video_info.get("time_base_rational").unwrap(),
            filter_spec.as_str(),
            is_hwaccel,
        );

        let graph = create_filters(&mut video, hw_format, filter_cfg)?;

        if video_params.rotation.abs() == 90 {
            std::mem::swap(&mut height, &mut width);
        }
        Ok(VideoDecoder::new(
            video, height, width, fps, video_info, is_hwaccel, graph,
        ))
    }

    pub fn decoder_start(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<(VideoReducer, usize), ffmpeg::Error> {
        let (reducer, start_frame, _end_frame) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        // check if first_index is after the first keyframe, if so we can seek
        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            self.seek_frame(&key_pos)?;
            self.curr_frame = key_pos;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let max_idx = reducer.get_indices().iter().max().unwrap_or(&0).to_owned();
        Ok((reducer, max_idx))
    }

    pub fn decode_next(&mut self) -> Result<FrameArray, ffmpeg::Error> {
        loop {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        self.decoder.video.send_packet(&packet)?;
                        match self.decoder.decode_frames()? {
                            Some(rgb_frame) => break Ok(rgb_frame),
                            None => continue,
                        };
                    }
                }
                None => {
                    if !self.draining {
                        self.decoder.video.send_eof()?;
                        self.draining = true;
                    }
                    match self.decoder.decode_frames()? {
                        Some(rgb_frame) => break Ok(rgb_frame),
                        None => {
                            self.draining = false;
                            self.decoder.video.flush();
                            self.seek_to_start()?;
                            break Err(ffmpeg::Error::Eof);
                        }
                    }
                }
            }
        }
    }

    pub fn decode_video(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<VideoArray, ffmpeg::Error> {
        let (mut reducer, max_idx) =
            self.decoder_start(start_frame, end_frame, compression_factor)?;
        for (stream, packet) in self.ictx.packets() {
            if reducer.get_frame_index() > max_idx {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                match self
                    .decoder
                    .receive_and_process_decoded_frames(&mut reducer)?
                {
                    Some(rgb_frame) => {
                        let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                        insert_frame(&mut slice_frame, rgb_frame);
                        reducer.incr_idx_counter(1);
                    }
                    None => debug!("No frame received!"),
                }
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        while !reducer.no_indices() && (reducer.get_frame_index() <= max_idx) {
            match self
                .decoder
                .receive_and_process_decoded_frames(&mut reducer)?
            {
                Some(rgb_frame) => {
                    let mut slice_frame = reducer.slice_mut(reducer.get_idx_counter());
                    slice_frame.zip_mut_with(&rgb_frame, |a, b| {
                        *a = *b;
                    });
                    reducer.incr_idx_counter(1);
                }
                None => {
                    debug!("No frame received!");
                    break;
                }
            }
        }

        // reset decoder
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(reducer.get_full_video())
    }

    pub async fn decode_video_fast(
        &mut self,
        start_frame: Option<usize>,
        end_frame: Option<usize>,
        compression_factor: Option<f64>,
    ) -> Result<Vec<FrameArray>, ffmpeg::Error> {
        let (reducer, start_frame, _) = VideoReducer::build(
            start_frame,
            end_frame,
            *self.stream_info.frame_count(),
            compression_factor,
            self.decoder.height,
            self.decoder.width,
        );
        let first_index = start_frame.unwrap_or(0);

        // make sure we are at the begining of the stream
        self.seek_to_start()?;

        if self
            .stream_info
            .key_frames()
            .iter()
            .any(|k| &first_index >= k)
            && (first_index > 0)
        {
            let key_pos = self.locate_keyframes(&first_index);
            // seek to closest key frame before first_index
            self.seek_frame(&key_pos)?;
        }

        let mut reducer = reducer.unwrap();
        reducer.set_frame_index(self.curr_frame);
        let mut tasks = vec![];

        let mut receive_and_process_decoded_frames = |decoder: &mut ffmpeg::decoder::Video,
                                                      mut curr_frame: usize|
         -> Result<usize, ffmpeg::Error> {
            let mut decoded = Video::empty();
            while decoder.receive_frame(&mut decoded).is_ok() {
                if reducer.get_indices().iter().any(|x| x == &curr_frame) {
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push(task::spawn(async move {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        } else {
                            tasks.push(task::spawn(async move {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }));
                        }
                    }
                }
                curr_frame += 1;
            }
            Ok(curr_frame)
        };

        for (stream, packet) in self.ictx.packets() {
            if &self.curr_frame > reducer.get_indices().iter().max().unwrap_or(&0) {
                break;
            }
            if stream.index() == self.stream_index {
                self.decoder.video.send_packet(&packet)?;
                let upd_curr_frame =
                    receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
                self.curr_frame = upd_curr_frame;
            } else {
                debug!("Packet for another stream");
            }
        }
        self.decoder.video.send_eof()?;
        // only process the remaining frames if we haven't reached the last frame
        if !reducer.no_indices()
            && (&self.curr_frame <= reducer.get_indices().iter().max().unwrap_or(&0))
        {
            let upd_curr_frame =
                receive_and_process_decoded_frames(&mut self.decoder.video, self.curr_frame)?;
            self.curr_frame = upd_curr_frame;
        }

        let mut outputs = Vec::with_capacity(tasks.len());
        for task_ in tasks {
            outputs.push(task_.await.unwrap());
        }

        // flush and go back to start
        self.decoder.video.flush();
        self.seek_to_start()?;

        Ok(outputs)
    }
    /// Safely get the batch of frames from the video by iterating over all frames and decoding
    /// only the ones we need. This can be more accurate when the video's metadata is not reliable,
    /// or when the video has B-frames.
    pub fn get_batch_safe(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        // Simple and robust implementation: iterate once from the start and grab the requested
        // frames in presentation order. This avoids any reliance on seeking or timestamp quirks
        // (B-frames, non-monotonic DTS/PTS, etc.).
        //
        // Complexity is O(total_frames) but only used when caller asks for the safe path.

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        // For fast membership checks
        let needed: HashSet<usize> = indices.iter().cloned().collect();
        let needed_total = needed.len();
        let max_needed = indices.iter().max().copied().unwrap_or(0);
        let mut frame_map: HashMap<usize, FrameArray> = HashMap::with_capacity(needed.len());
        let mut tasks: Vec<(usize, thread::JoinHandle<FrameArray>)> =
            Vec::with_capacity(needed.len());

        let mut decoded = Video::empty();
        let mut curr_idx: usize = 0;
        let mut collected: usize = 0;

        // iterate all packets
        'packets: for (stream, packet) in self.ictx.packets() {
            if stream.index() != self.stream_index {
                continue;
            }
            self.decoder.video.send_packet(&packet)?;
            while self.decoder.video.receive_frame(&mut decoded).is_ok() {
                if needed.contains(&curr_idx) {
                    // push frame through filter graph, offload color conversion to async task
                    self.decoder
                        .graph
                        .get("in")
                        .unwrap()
                        .source()
                        .add(&decoded)
                        .unwrap();
                    let mut rgb_frame = Video::empty();
                    if self
                        .decoder
                        .graph
                        .get("out")
                        .unwrap()
                        .sink()
                        .frame(&mut rgb_frame)
                        .is_ok()
                    {
                        let cspace = self.decoder.color_space;
                        let crange = self.decoder.color_range;
                        if self.decoder.is_hwaccel {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        } else {
                            tasks.push((
                                curr_idx,
                                thread::spawn(move || {
                                    convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                                }),
                            ));
                        }
                        collected += 1;
                    }
                }
                curr_idx += 1;
                if collected >= needed_total || curr_idx > max_needed {
                    break 'packets;
                }
            }
        }
        // flush remaining frames
        self.decoder.video.send_eof()?;
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            if needed.contains(&curr_idx) {
                self.decoder
                    .graph
                    .get("in")
                    .unwrap()
                    .source()
                    .add(&decoded)
                    .unwrap();
                let mut rgb_frame = Video::empty();
                if self
                    .decoder
                    .graph
                    .get("out")
                    .unwrap()
                    .sink()
                    .frame(&mut rgb_frame)
                    .is_ok()
                {
                    let cspace = self.decoder.color_space;
                    let crange = self.decoder.color_range;
                    if self.decoder.is_hwaccel {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_nv12_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    } else {
                        tasks.push((
                            curr_idx,
                            thread::spawn(move || {
                                convert_yuv_to_ndarray_rgb24(rgb_frame, cspace, crange)
                            }),
                        ));
                    }
                    collected += 1;
                }
            }
            curr_idx += 1;
            if collected >= needed_total || curr_idx > max_needed {
                break;
            }
        }

        // gather async conversions
        for (idx, task_) in tasks {
            let frame = task_.join().unwrap();
            frame_map.insert(idx, frame);
        }

        // Build output in the same order as requested (including duplicates)
        let mut frame_batch: VideoArray = Array4::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        for (out_i, idx) in indices.iter().enumerate() {
            if let Some(frame) = frame_map.get(idx) {
                frame_batch
                    .slice_mut(s![out_i, .., .., ..])
                    .assign(frame);
            } else {
                debug!("No frame found for {}", idx);
            }
        }

        // reset decoder state for subsequent calls
        self.decoder.video.flush();
        self.seek_to_start()?;
        Ok(frame_batch)
    }

    /// Get the batch of frames from the video by seeking to the closest keyframe and skipping
    /// the frames until we reach the desired frame index. Heavily inspired by the implementation
    /// from decord library: https://github.com/dmlc/decord
    pub fn get_batch(&mut self, indices: Vec<usize>) -> Result<VideoArray, ffmpeg::Error> {
        let mut video_frames: VideoArray = Array::zeros((
            indices.len(),
            self.decoder.height as usize,
            self.decoder.width as usize,
            3,
        ));

        // make sure we are at the beginning of the stream
        self.seek_to_start()?;

        for (idx_counter, frame_index) in indices.into_iter().enumerate() {
            self.n_fails = 0;
            debug!("[NEXT INDICE] frame_index: {frame_index}");
            self.seek_accurate(
                frame_index,
                &mut video_frames.slice_mut(s![idx_counter, .., .., ..]),
            )?;
        }
        Ok(video_frames)
    }


    pub fn seek_accurate(
        &mut self,
        frame_index: usize,  // This is a packet/decode order index (iterator output order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        // Apply offset for negative PTS frames
        let offset = self.stream_info.negative_pts_offset();
        let adjusted_index = frame_index.saturating_add(offset);

        // Find the keyframe before the adjusted index
        let key_decode_idx = self.locate_keyframes(&adjusted_index);

        debug!(
            "    - Target decode idx: {} (adjusted: {}), keyframe decode idx: {}, offset: {}",
            frame_index, adjusted_index, key_decode_idx, offset
        );
        debug!(
            "    - curr_frame: {}, curr_dec_idx: {}",
            self.curr_frame, self.curr_dec_idx
        );

        // Check if we can skip forward without seeking
        // We can only skip if we're already at or past the keyframe for the target
        let can_skip_forward = (adjusted_index >= self.curr_dec_idx)
            && (self.curr_dec_idx >= key_decode_idx);

        if can_skip_forward {
            // we can directly skip until adjusted_index
            debug!("No need to seek, we can directly skip frames");
            let num_skip = adjusted_index.saturating_sub(self.curr_dec_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        } else {
            debug!("Seeking to keyframe at decode idx: {}", key_decode_idx);
            // For backwards seeking, first go to start then seek forward
            if key_decode_idx < self.curr_dec_idx {
                debug!("Backward seek detected, going to start first");
                self.seek_to_start()?;
            }
            self.seek_frame_by_decode_idx(&key_decode_idx)?;
            self.curr_frame = key_decode_idx;
            let num_skip = adjusted_index.saturating_sub(key_decode_idx);
            match self.skip_frames(num_skip, &frame_index, frame_array) {
                Ok(()) => Ok(()),
                Err(_) => self.get_frame_after_eof(frame_array, &frame_index),
            }
        }
    }

    /// Find the closest key frame before `pos`
    pub fn locate_keyframes(&self, pos: &usize) -> usize {
        let key_pos = self
            .stream_info
            .key_frames()
            .iter()
            .filter(|e| &pos >= e)
            .max()
            .unwrap_or(&0);
        key_pos.to_owned()
    }

    /// How many frames we need to skip to go from current decoding index `curr_dec_idx`
    /// to `target_dec_index`. The `frame_index` argument corresponds to the presentation
    /// index, while we need to know the number of frames to skip in terms of decoding index.
    pub fn get_num_skip(&self, frame_index: &usize) -> usize {
        frame_index.saturating_sub(self.curr_dec_idx)
    }

    /// Seek back to the begining of the stream
    fn seek_to_start(&mut self) -> Result<(), ffmpeg::Error> {
        self.ictx.seek(0, ..100)?;
        self.avflushbuf()?;
        self.curr_dec_idx = 0;
        self.curr_frame = 0;
        Ok(())
    }

    pub fn skip_frames(
        &mut self,
        num: usize,
        frame_index: &usize,  // decode order index (packet order)
        frame_array: &mut ArrayViewMut3<u8>,
    ) -> Result<(), ffmpeg::Error> {
        let num_skip = num.min(self.stream_info.frame_count() - 1);
        debug!(
            "will skip {} frames, from curr_dec_idx:{}, target: {}",
            num_skip, self.curr_dec_idx, frame_index
        );
        // For B-frame videos, we may need more iterations
        // Use a larger failsafe based on GOP size estimation (usually ~30 frames per GOP)
        let mut failsafe = ((num_skip + 30) * 2) as i32;
        let mut first_packet = true;

        while failsafe > -1 {
            match self.ictx.packets().next() {
                Some((stream, packet)) => {
                    if stream.index() == self.stream_index {
                        if first_packet {
                            debug!(
                                "First packet after seek: pts={:?}, dts={:?}, is_key={}",
                                packet.pts(),
                                packet.dts(),
                                packet.is_key()
                            );
                            first_packet = false;
                        }
                        self.decoder.video.send_packet(&packet)?;
                        let (rgb_frame, counter) = self.get_frame(frame_index);
                        if let Some(frame) = rgb_frame {
                            insert_frame(frame_array, frame);
                            return Ok(());
                        }
                        failsafe -= counter
                    }
                }
                None => {
                    debug!("No more packet!");
                    return Err(ffmpeg::Error::Eof);
                }
            }
        }
        debug!(
            "Finished skipping, current dec idx is now: {}",
            self.curr_dec_idx
        );
        Err(ffmpeg::Error::Eof)
    }

    /// Get frame at `frame_index` when there is no more packets to iterate
    pub fn get_frame_after_eof(
        &mut self,
        frame_array: &mut ArrayViewMut3<u8>,
        frame_index: &usize,
    ) -> Result<(), ffmpeg::Error> {
        self.decoder.video.send_eof()?;
        let (rgb_frame, _counter) = self.get_frame(frame_index);
        if let Some(frame) = rgb_frame {
            insert_frame(frame_array, frame);
        }
        Ok(())
    }

    /// Get a single frame at `frame_index` (decode order index, matching iterator output)
    /// After seeking to keyframe K, the N-th decoded frame is at position K+N-1.
    /// We count frames to find the target position, with offset correction for negative PTS frames.
    pub fn get_frame(&mut self, frame_index: &usize) -> (Option<FrameArray>, i32) {
        // Apply offset correction for videos with negative PTS frames
        // After seeking, frames are offset by the number of negative PTS frames
        let offset = self.stream_info.negative_pts_offset();
        let adjusted_target = frame_index.saturating_add(offset);

        let mut decoded = Video::empty();
        let mut counter = 0;
        let mut rgb_frame: Option<FrameArray> = None;

        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            let pts = decoded.pts().unwrap_or(-1);

            debug!(
                "Decoded frame: pts={}, curr_dec_idx={}, target={} (adjusted={})",
                pts, self.curr_dec_idx, frame_index, adjusted_target
            );

            // Match by position count with offset adjustment
            // After seek to keyframe K, curr_dec_idx starts at K
            // Each decoded frame increments curr_dec_idx
            // We need to match at adjusted_target to account for negative PTS frames
            if self.curr_dec_idx == adjusted_target {
                debug!(
                    "Found target frame {} (adjusted {}) at position {}",
                    frame_index, adjusted_target, self.curr_dec_idx
                );
                rgb_frame = self.decoder.process_frame(&decoded);
            }

            self.curr_dec_idx += 1;
            self.curr_frame = self.curr_dec_idx;
            counter += 1;
        }
        (rgb_frame, counter)
    }

    /// Update the current frame index and decoding index
    pub fn update_indices(&mut self) {
        self.curr_dec_idx += 1;
        self.curr_frame = self.curr_dec_idx;
        debug!(
            "dec_idx: {}, curr_frame: {}",
            self.curr_dec_idx, self.curr_frame
        );
    }

    // AVSEEK_FLAG_BACKWARD 1 <- seek backward
    // AVSEEK_FLAG_BYTE 2 <- seeking based on position in bytes
    // AVSEEK_FLAG_ANY 4 <- seek to any frame, even non-key frames
    // AVSEEK_FLAG_FRAME 8 <- seeking based on frame number
    /// Seek to a decode order position (packet index)
    pub fn seek_frame_by_decode_idx(&mut self, decode_idx: &usize) -> Result<(), ffmpeg::Error> {
        if let Some(fr_ts) = self.stream_info.frame_times().get(decode_idx) {
            let pts = *fr_ts.pts();
            let dts = *fr_ts.dts();

            debug!(
                "seek_frame_by_decode_idx: decode_idx={}, pts={}, dts={}",
                decode_idx, pts, dts
            );

            // Try seeking with just the timestamp, no flags (FFmpeg will find nearest keyframe)
            // Using PTS as it's more commonly used for seeking
            match self.avseekframe(decode_idx, pts, 0) {
                Ok(()) => {
                    debug!("seek_frame_by_decode_idx: seek with pts={} succeeded", pts);
                    Ok(())
                }
                Err(_) => {
                    // Try with DTS
                    debug!("seek_frame_by_decode_idx: trying with dts={}", dts);
                    self.avseekframe(decode_idx, dts, 0)
                }
            }
        } else {
            debug!("seek_frame_by_decode_idx: decode_idx={}, no frame_times entry", decode_idx);
            Err(ffmpeg::Error::Bug)
        }
    }

    // Legacy seek_frame for compatibility
    pub fn seek_frame(&mut self, pos: &usize) -> Result<(), ffmpeg::Error> {
        self.seek_frame_by_decode_idx(pos)
    }
    pub fn avseekframe(
        &mut self,
        pos: &usize,
        frame_ts: i64,
        flag: i32,
    ) -> Result<(), ffmpeg::Error> {
        let res = unsafe {
            av_seek_frame(
                self.ictx.as_mut_ptr(),
                self.stream_index as i32,
                frame_ts,
                flag,
            )
        };
        self.avflushbuf()?;
        if res >= 0 {
            self.curr_dec_idx = *pos;
            self.curr_frame = *pos;
            Ok(())
        } else {
            Err(ffmpeg::Error::from(res))
        }
    }

    pub fn avflushbuf(&mut self) -> Result<(), ffmpeg::Error> {
        // Flush decoder buffers to discard any pending frames
        unsafe { avcodec_flush_buffers(self.decoder.video.as_mut_ptr()) };
        // Also drain any remaining frames from the decoder
        let mut decoded = Video::empty();
        while self.decoder.video.receive_frame(&mut decoded).is_ok() {
            // Discard buffered frames
            debug!("Discarding buffered frame after flush");
        }
        Ok(())
    }
}

impl Iterator for VideoReader {
    type Item = FrameArray;
    fn next(&mut self) -> Option<Self::Item> {
        self.decode_next().ok()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ffmpeg::format::input;
    use ffmpeg::media::Type;
    use std::path::Path;

    const TEST_VIDEO: &'static str = "./assets/input.mp4";
    const TEST_AUDIO: &'static str = "./assets/audio_only.mp3";
    const NO_FILE: &'static str = "./assets/non_existent_file.mp4";

    #[test]
    fn test_get_init_context_success() {
        let filename = String::from(TEST_VIDEO);
        let result = get_init_context(&filename);
        assert!(result.is_ok());

        let (ctx, stream_index) = result.unwrap();
        assert!(stream_index < ctx.streams().count());

        let stream = ctx.streams().find(|s| s.index() == stream_index);
        assert!(stream.is_some());
        let stream = stream.unwrap();
        assert_eq!(stream.parameters().medium(), Type::Video);
    }

    #[test]
    fn test_get_init_context_file_not_found() {
        let filename = String::from(NO_FILE);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_get_init_context_no_video_stream() {
        // This test requires a file with no video streams, like an audio-only file
        let filename = String::from(TEST_AUDIO);
        let result = get_init_context(&filename);
        assert!(result.is_err());
    }

    #[test]
    fn test_setup_decoder_context_no_hwaccel() {
        let path = Path::new(TEST_VIDEO);
        ffmpeg::init().unwrap();
        let ictx = input(&path).unwrap();
        let stream = ictx
            .streams()
            .best(Type::Video)
            .expect("No video stream found");

        // test with multiple threads
        let result = setup_decoder_context(&stream, 4, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with 1 thread
        let result = setup_decoder_context(&stream, 1, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());

        // Test with threads set to 0
        let result = setup_decoder_context(&stream, 0, None);
        assert!(result.is_ok());
        let (context, hwaccel) = result.unwrap();
        assert!(hwaccel.is_none());
        assert!(context.decoder().video().is_ok());
    }
}

```

🎉 **终于成功了！** `with_fallback=False: True`

让我用更多索引测试确保修复可靠：

```python
import os
from time import time
import numpy as np
from PIL import Image
from video_reader import PyVideoReader
import math
from tqdm import tqdm
import contextlib
import sys
import time
import logging

# Enable debug logging for the rust library
os.environ['RUST_LOG'] = 'debug'

vid_path = '/Users/bytedance/Downloads/ea09afcd-425a-499e-86c3-a88d0b1d70c8.mov'

batch_indexs = [73, 87, 111, 172, 174, 192, 214, 227, 235, 238, 253, 276, 405, 417, 433, 480, 509, 527, 532, 532, 535, 539, 541, 543, 577, 579, 606, 606, 635, 644, 645, 689]
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

# Full test with original indices
batch_indexs = [63, 78, 140, 180, 221, 226, 339, 355, 393, 407, 437, 444, 445, 465, 466, 468, 498, 498, 505, 532, 536, 546, 548, 556, 574, 576, 584, 592, 604, 615, 628, 701]

start_time = time.time()
vr_rs = PyVideoReader(vid_path)
# Get PTS info first
pts_list = vr_rs.get_pts()
print(f"Total frames: {len(vr_rs)}")
print(f"First 20 PTS values: {pts_list[:20]}")

# Check PTS order (are they sorted?)
is_sorted = all(pts_list[i] <= pts_list[i+1] for i in range(len(pts_list)-1))
print(f"PTS values are sorted: {is_sorted}")

# Check what the 63rd frame's PTS actually is in ground_truth order (packet order)
print(f"PTS of ground_truth[63] (packet order): {pts_list[63]:.4f} = {pts_list[63] * 600:.0f} timestamp units")
print(f"PTS of ground_truth[56] (packet order): {pts_list[56]:.4f} = {pts_list[56] * 600:.0f} timestamp units")

# Print PTS sequence for frames 55-70
print("\nPTS sequence for frames 55-70 (packet order):")
for i in range(55, 71):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f} = {pts_list[i] * 600:.0f} ts")

# Check if iterator output is in PTS sorted order by examining the actual frames
# This is the key test: is ground_truth[N] the frame with (N+1)th smallest PTS?
sorted_pts = sorted(enumerate(pts_list), key=lambda x: x[1])
print("\nFirst 10 frames in sorted PTS order:")
for rank, (packet_idx, pts_val) in enumerate(sorted_pts[:10]):
    print(f"  rank {rank}: packet {packet_idx}, PTS = {pts_val:.4f}")

# Count frames with negative PTS
negative_pts_count = sum(1 for pts in pts_list if pts < 0)
print(f"\nNumber of frames with negative PTS: {negative_pts_count}")

# Check which packet corresponds to ground_truth[63]
# If iterator outputs in PTS order, ground_truth[63] should match packet at sorted_pts[63]
print(f"\nSorted PTS position 56: packet {sorted_pts[56][0]}, PTS = {sorted_pts[56][1]:.4f}")
print(f"Sorted PTS position 63: packet {sorted_pts[63][0]}, PTS = {sorted_pts[63][1]:.4f}")

# Check PTS around keyframe 56
print(f"PTS around keyframe 56 (packets 50-65):")
for i in range(50, 66):
    print(f"  packet {i}: PTS = {pts_list[i]:.4f}")

# Check video info
info = vr_rs.get_info()
print(f"\nVideo info:")
print(f"  start_time: {info.get('start_time', 'N/A')}")
print(f"  time_base: {info.get('time_base', 'N/A')}")

ground_truth = np.array([frame for frame in vr_rs])
batch_gt = ground_truth[batch_indexs]
end_time = time.time()
print(f"Time taken to get ground truth: {end_time - start_time:.2f} seconds")

start_time = time.time()
vr = PyVideoReader(vid_path)
batch_arr = vr.get_batch(batch_indexs, with_fallback=False)
end_time = time.time()
print(f"Time taken to get batch (with_fallback=False): {end_time - start_time:.2f} seconds")
print(f"with_fallback=False: {np.array_equal(batch_arr, batch_gt)}")

# Compare first frame in detail
print(f"\nFirst index: {batch_indexs[0]}")
print(f"Ground truth shape: {batch_gt[0].shape}, sum: {batch_gt[0].sum()}")
print(f"Batch arr shape: {batch_arr[0].shape}, sum: {batch_arr[0].sum()}")
print(f"First frame match: {np.array_equal(batch_arr[0], batch_gt[0])}")

# Check which frames match
for i, idx in enumerate(batch_indexs):
    if not np.array_equal(batch_arr[i], batch_gt[i]):
        print(f"Frame {idx} mismatch: batch sum={batch_arr[i].sum()}, gt sum={batch_gt[i].sum()}")
        # Try to find which ground truth frame matches (search wider range)
        for j in range(max(0, idx-60), min(len(ground_truth), idx+60)):
            if np.array_equal(batch_arr[i], ground_truth[j]):
                print(f"  -> Actually matches ground_truth[{j}] (offset: {idx - j})")
                break
        else:
            print(f"  -> No match found in range [{max(0,idx-60)}, {min(len(ground_truth), idx+60)})")

start_time = time.time()
vr2 = PyVideoReader(vid_path)
batch_arr2 = vr2.get_batch(batch_indexs, with_fallback=True)
end_time = time.time()
print(f"\nTime taken to get batch (with_fallback=True): {end_time - start_time:.2f} seconds")
print(f"with_fallback=True: {np.array_equal(batch_arr2, batch_gt)}")
```

🎉 **成功！** 所有 32 个帧索引都正确匹配了：
- `with_fallback=False: True` ✅
- `with_fallback=True: True` ✅

让我更新 TODO 并清理代码中的警告：

太好了，修复已完成！

## 总结

问题的根本原因是：**在含有负 PTS 帧的 B 帧视频中，FFmpeg seek 后的帧输出顺序与从头迭代时有固定偏移。**

具体来说：
1. 视频开头有 7 帧的 PTS 是负数（小于 0）
2. 当 seek 到某个 keyframe 后开始解码，输出的帧与从头迭代相比有 7 帧的偏移
3. 这导致 `with_fallback=False` 时返回的帧内容不匹配 ground truth

**修复方案**：
1. 在 `StreamInfo` 中计算并存储负 PTS 帧的数量（`negative_pts_offset`）
2. 在 `seek_accurate` 和 `get_frame` 方法中，将目标帧索引加上这个偏移量来校正

关键代码修改：
- `src/info.rs`: 添加 `negative_pts_offset` 字段和计算逻辑
- `src/reader.rs`: 在 `seek_accurate` 和 `get_frame` 中应用偏移校正

